---
title: Engineering & Production (AI 工程与生产落地)
date: 2025-01-04
focus: [工程实践, 系统设计, 成本优化, 质量保证]
maturity: [快速成熟, 最佳实践涌现]
---

# 🛠️ Engineering & Production: AI 从理论到实战

## 引言

模型很强，但真正的 AI 系统是由以下部分组成的：

```
理想状态：一个聪明的 LLM
│
↓

现实情况：
  ┌─────────────────────────────────────────┐
  │  UI/UX 层（用户看到的）                 │
  ├─────────────────────────────────────────┤
  │  应用业务逻辑层                         │
  ├─────────────────────────────────────────┤
  │  提示词工程 + 上下文管理                 │
  ├─────────────────────────────────────────┤
  │  RAG 检索增强（外部知识）                │
  ├─────────────────────────────────────────┤
  │  LLM API 调用层（带重试、缓存）         │
  ├─────────────────────────────────────────┤
  │  评估框架（衡量质量）                   │
  ├─────────────────────────────────────────┤
  │  监控与日志（生产可观测性）             │
  ├─────────────────────────────────────────┤
  │  成本控制与优化层                       │
  └─────────────────────────────────────────┘
```

本部分讨论的是上面这个「完整的系统」，而不是单纯的模型。

---

## 第一部分：工程实践的四个关键领域

### 领域 1：RAG（检索增强生成）

**问题**：LLM 的知识库是固定的（GPT-4o 的数据只到 2024 年 4 月）

**解决方案**：在提问前先检索相关文档，注入到 Prompt 中

```
用户问题：「我们公司 2024 年 Q4 的财务表现如何？」

不用 RAG：
  LLM：「我不知道你公司的最新财务数据」

用 RAG：
  1. 检索公司内部文档（2024 Q4 财报）
  2. 注入 Prompt：「基于以下财报数据...」
  3. LLM：「根据财报，Q4 收入增长了 25%...」

成果：LLM 用最新的私有数据回答问题
```

**成熟度**：⭐⭐⭐⭐☆（快速成熟）
**应用范围**：企业知识库、长文本理解、实时数据集成
**成本影响**：向量数据库 + 检索延迟（平均 200-500ms）

### 领域 2：提示词工程（Prompt Engineering）

**问题**：同一个 LLM，不同的提示词，结果差异巨大

```
坏提示词：「总结这篇文章」
结果：「这篇文章讲了很多东西」（没有用）

好提示词：「用三个要点总结这篇文章，每个要点不超过 20 字」
结果：「1. 气候变化...2. 能源转型...3. 政策影响...」

提示词工程的目标：
  通过精心设计提示词，让 LLM 输出更准确、更有用的结果
```

**成熟度**：⭐⭐⭐⭐⭐（已成熟最佳实践）
**应用范围**：任何使用 LLM 的场景
**成本影响**：几乎零（只是设计时间）

### 领域 3：评估框架（Evaluation）

**问题**：怎么知道 LLM 的输出是好是坏？

```
例子 1：文本生成
  输出：「天气真好」
  问题：符合语法吗？（✓）通顺吗？（✓）回答了问题吗？（？）
  需要多个指标来评价

例子 2：代码生成
  输出：一段 Python 代码
  评估：语法正确？（✓）能运行吗？（✓）符合需求吗？（✓）性能如何？（？）

成熟的评估框架 = 定义清晰的指标 + 自动化测试
```

**成熟度**：⭐⭐⭐⭐（快速成熟）
**应用范围**：模型选择、版本对比、持续改进
**成本影响**：评估成本（通常是 LLM API 调用）

### 领域 4：监控与可观测性（Monitoring & Observability）

**问题**：LLM 应用上线后，怎么知道它运行得好不好？

```
传统应用监控：
  - 请求延迟（Latency）
  - 错误率（Error Rate）
  - CPU/内存使用

AI 应用的额外监控：
  - Token 消耗（成本）
  - LLM 输出质量（是否有幻觉？）
  - 用户反馈（人工评分）
  - 缓存命中率（成本优化）

目标：实时发现问题，快速迭代改进
```

**成熟度**：⭐⭐⭐☆☆（逐步成熟）
**应用范围**：生产系统的持续运维
**成本影响**：日志存储 + 分析工具

---

## 第二部分：工程实践矩阵

### 按系统复杂度

```
简单系统：
  用户 → LLM API → 回复
  需要：好的提示词
  不需要：RAG、复杂评估、监控

中等复杂：
  用户 → 提示词优化 + 简单检索 → LLM → 回复
  需要：提示词工程、基础 RAG、简单评估

复杂系统：
  用户 → Agent（决策）→ 多个 LLM + 检索 + 工具调用 → 回复
  需要：完整的 RAG、评估框架、监控告警、A/B 测试
```

### 按成本敏感度

```
成本不敏感（大企业、高价值任务）：
  ✓ 用最强的模型（GPT-4o）
  ✓ 完整的评估和监控
  ✗ 不需要过度优化 token 使用

成本敏感（创业、边缘场景）：
  ✓ 用便宜的模型（Claude Haiku、DeepSeek）
  ✓ 优化 token 使用（缓存、压缩提示词）
  ✗ 可能牺牲一些质量
```

### 按质量要求

```
低质量要求（内部工具、辅助作用）：
  提示词 → 直接用 → 没关系如果有小错误
  成熟度：简单

中等质量（面向终端用户）：
  提示词 + 评估 + 反馈循环 → 逐步改进
  成熟度：中等

高质量（医疗、金融、法律）：
  完整的 RAG + 强化评估 + 专家审核 + 法规遵从
  成熟度：复杂
```

---

## 第三部分：工程实践的关键指标

### 成本指标

```
单个请求成本 = (输入 tokens × 输入价格 + 输出 tokens × 输出价格) × 优化系数

优化机制：
  缓存：减少 20-50% 的 token 消耗
  蒸馏：用小模型替代大模型（省 80% 成本，牺牲 5-10% 质量）
  批处理：减少延迟但提高吞吐

例子：
  原成本：1000 请求 × 0.01 美元 = $10
  用缓存：1000 请求 × 0.005 美元 = $5 (省 50%)
  用小模型：1000 请求 × 0.002 美元 = $2 (省 80%)
```

### 质量指标

```
自动化指标：
  • BLEU / ROUGE / METEOR（文本相似度）
  • Exact Match（精确匹配）
  • Token Accuracy（token 级别准确度）

启发式指标：
  • 幻觉检测（输出是否真的基于输入）
  • 相关性评分（回答是否相关）
  • 流畅性评分（文本是否自然）

人工指标：
  • 用户满意度（1-5 星）
  • 专家评分（领域专家打分）
  • A/B 测试结果
```

### 延迟指标

```
P50 延迟（中位数）：用户大多数时间的等待
P95 延迟（95%分位数）：用户偶尔遇到的最长等待
P99 延迟（99%分位数）：最坏情况

目标：
  简单任务：P95 < 2 秒
  中等任务：P95 < 5 秒
  复杂任务：P95 < 10 秒

改进方法：
  缓存（减少重复调用）
  流式输出（显示部分结果）
  并行调用（同时调用多个 API）
```

---

## 第四部分：2025 年的工程趋势

### 趋势 1：从孤立工具到集成框架

**现在（2024）**：
```
ChatGPT → 写文案
Cursor → 写代码
Perplexity → 搜索信息
各个工具分离，用户要在不同应用间切换
```

**2025 年**：
```
集成平台（如 Claude Code、VS Code Agent）
一个环境内：
  - 代码生成
  - 文档搜索
  - 实时测试
  - 代码审查
不再需要切换
```

### 趋势 2：从 API 到 Agent

**API 调用模式**（现在）：
```
用户输入 → LLM → 固定的输出格式 → 完成
一次性交互，有时候不够灵活
```

**Agent 模式**（2025）：
```
用户输入 → Agent（思考和规划）→ 可能调用多个工具 → 反思 → 修正 → 输出
多步交互，自适应问题解决
```

### 趋势 3：成本优化会成为核心竞争力

**原因**：
- 模型能力趋于接近
- 差异化转向成本和速度

**优化方向**：
- 更便宜的模型（DeepSeek、Llama）
- 缓存策略（KV Cache、提示词缓存）
- 小模型蒸馏（大模型 → 小模型）
- 混合策略（简单问题用小模型，复杂问题用大模型）

**效果**：
```
2024 年：GPT-4o = $0.030/1K 输入 tokens
2025 年预测：
  通用模型 = $0.005/1K tokens（成本下降 6 倍）
  开源模型 = $0 + 计算成本（自托管）
```

### 趋势 4：监控会成为必须

**原因**：
- LLM 输出不可预测（今天好的提示词，明天可能不行）
- 生产系统需要实时反馈

**2025 年的标准做法**：
- 每个 LLM 应用都有质量监控
- 自动告警（输出质量下降时）
- 持续的 A/B 测试（对比不同的模型、提示词）
- 用户反馈闭环

---

## 第五部分：选择工程框架

### 按使用场景

```
场景 1：简单聊天应用
  需要：好的提示词
  框架：LangChain（最简单的 RAG）
  成本：最小化
  例子：内部 Slack Bot

场景 2：企业知识库（FAQ）
  需要：RAG + 提示词优化
  框架：LangChain + Pinecone/Weaviate
  成本：中等（向量数据库）
  例子：客服知识库

场景 3：代码生成或复杂任务
  需要：多模型编排、工具调用、反思
  框架：LangGraph / LlamaIndex
  成本：高（多次 API 调用）
  例子：Cursor、代码审查系统

场景 4：生产级应用（金融、医疗）
  需要：完整的 RAG + 强大的评估 + 审计日志
  框架：自建或 LlamaIndex Enterprise
  成本：很高（但值得）
  例子：医疗诊断辅助
```

### 主流工程框架对比

```
           LangChain  LlamaIndex  LangGraph  AutoGen
易用性      ⭐⭐⭐⭐⭐  ⭐⭐⭐⭐   ⭐⭐⭐   ⭐⭐⭐
RAG 功能    ⭐⭐⭐⭐  ⭐⭐⭐⭐⭐  ⭐⭐⭐   ⭐⭐⭐
Agent 支持  ⭐⭐⭐⭐  ⭐⭐⭐⭐   ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐⭐
社区规模    ⭐⭐⭐⭐⭐  ⭐⭐⭐⭐   ⭐⭐⭐   ⭐⭐⭐
最适合      快速原型  RAG 系统  Agent 编排  多 Agent
```

---

## 总结

工程实践的核心是**让 AI 系统在生产环境中可靠、经济、高效地运行**。

### 四个必须做的事情

```
1. 提示词工程（必做）
   → 同样的模型，提示词好坏决定了效果

2. 评估框架（必做）
   → 你得知道系统表现如何

3. RAG 或类似的上下文增强（通常需要）
   → 除非你只用通用知识

4. 监控（生产必做）
   → 问题一定会出现，关键是快速发现和修复
```

### 工程实践成熟度总结

```
提示词工程：⭐⭐⭐⭐⭐ 最成熟，最佳实践清晰
RAG 系统：⭐⭐⭐⭐☆ 成熟度高，工具完善
评估框架：⭐⭐⭐⭐☆ 快速成熟，标准涌现
监控可观测性：⭐⭐⭐☆☆ 逐步成熟，还需探索
```

### 对企业的建议

```
小公司 / 创业：
  1. 从提示词工程开始（最高 ROI）
  2. 加上简单的 RAG（LangChain）
  3. 定性评估（用户反馈）
  成本：$200-500/月

中等企业：
  1. 完整的提示词优化流程
  2. 生产级 RAG（Pinecone / Weaviate）
  3. 自动化评估 + A/B 测试
  4. 基础监控
  成本：$5K-20K/月

大企业：
  1. 定制的提示词工程团队
  2. 企业级 RAG + 多源知识融合
  3. 强大的评估框架（超过 20 个指标）
  4. 完整的可观测性 + 告警 + 自动化
  成本：$100K+/月，但 ROI 可能 10 倍以上
```

---

## 关键链接

- **[[RAG_Systems]]** - 深入 RAG 系统设计
- **[[Prompt_Engineering]]** - 提示词工程最佳实践
- **[[Evaluation_Methods]]** - 评估框架和指标
- **[[Monitoring_Observability]]** - 监控和可观测性

