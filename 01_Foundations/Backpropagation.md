---
topic: Backpropagation (åå‘ä¼ æ’­ç®—æ³•)
category: Optimization
difficulty: â­â­â­â­â­
tags: [åŸºç¡€, åŸç†, å¿…ä¿®, ä¼˜åŒ–]
---

# ğŸ”„ Backpropagation Algorithm

## 1. ğŸ“– æœ¬è´¨å®šä¹‰ (First Principles)

> [!SUMMARY] æ¦‚å¿µå¡ç‰‡
>
> - **å®šä¹‰**ï¼šåå‘ä¼ æ’­æ˜¯ä¸€ç§é«˜æ•ˆè®¡ç®—ç¥ç»ç½‘ç»œä¸­æ‰€æœ‰å‚æ•°æ¢¯åº¦çš„ç®—æ³•ã€‚é€šè¿‡**é“¾å¼æ³•åˆ™**ä»æŸå¤±å‡½æ•°å‡ºå‘ï¼Œé€å±‚è®¡ç®—æ¯ä¸ªå‚æ•°å¯¹æŸå¤±çš„åå¯¼æ•°ï¼Œç„¶åç”¨æ¢¯åº¦ä¸‹é™æ›´æ–°å‚æ•°ã€‚
>
> - **å†å²èƒŒæ™¯**ï¼šåœ¨åå‘ä¼ æ’­æå‡ºä¹‹å‰ï¼ˆ1970sï¼‰ï¼Œæ·±åº¦ç¥ç»ç½‘ç»œæ— æ³•æœ‰æ•ˆè®­ç»ƒï¼Œå› ä¸ºè®¡ç®—æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦éœ€è¦ O(å‚æ•°æ•°é‡) çš„å¤æ‚è®¡ç®—ã€‚åå‘ä¼ æ’­çš„å…³é”®åˆ›æ–°æ˜¯ï¼š**å¤ç”¨ä¸­é—´è®¡ç®—ç»“æœ**ï¼Œä½¿å¾—è®¡ç®—æ‰€æœ‰æ¢¯åº¦çš„æ€»æˆæœ¬ä»…ä¸º**æ­£å‘ä¼ æ’­çš„å¸¸æ•°å€**ï¼ˆçº¦ 3 å€ï¼‰ã€‚
>
> - **æ ¸å¿ƒç›´è§‰**ï¼šå°±åƒç‰©æµç³»ç»Ÿä¸­çš„"å›ç¨‹"ï¼Œæ­£å‘ä¼ æ’­è®¡ç®—è¾“å‡ºæ—¶ä¼šäº§ç”Ÿå¾ˆå¤šä¸­é—´ç»“æœã€‚åå‘ä¼ æ’­å·§å¦™åœ°åœ¨"å›ç¨‹"æ—¶é‡ç”¨è¿™äº›ç»“æœï¼Œé«˜æ•ˆåœ°è®¡ç®—æ¢¯åº¦ã€‚

### âš¡ ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ

```
æ·±åº¦ç¥ç»ç½‘ç»œçš„è®­ç»ƒæµç¨‹ï¼š
1. æ­£å‘ä¼ æ’­ï¼šè¾“å…¥ xï¼Œè®¡ç®—é¢„æµ‹ Å·
2. è®¡ç®—æŸå¤±ï¼šL(y, Å·)
3. åå‘ä¼ æ’­ï¼šè®¡ç®— âˆ‡Lï¼ˆæ‰€æœ‰å‚æ•°çš„æ¢¯åº¦ï¼‰
4. æ¢¯åº¦ä¸‹é™ï¼šÎ¸ â† Î¸ - Î±âˆ‡L

åå‘ä¼ æ’­ç›´æ¥å†³å®šäº†æ˜¯å¦èƒ½åœ¨æœ‰é™æ—¶é—´å†…è®­ç»ƒæ¨¡å‹ï¼
```

---

## 2. ğŸ“ æ•°å­¦æ¨å¯¼ (The Math)

### 2.1 é“¾å¼æ³•åˆ™ï¼ˆChain Ruleï¼‰ï¼šåå‘ä¼ æ’­çš„åŸºç¡€

å¯¹äºå¤åˆå‡½æ•° $f(g(h(x)))$ï¼Œé“¾å¼æ³•åˆ™å‘Šè¯‰æˆ‘ä»¬ï¼š

$$\frac{df}{dx} = \frac{df}{dg} \cdot \frac{dg}{dh} \cdot \frac{dh}{dx}$$

**ç¥ç»ç½‘ç»œçš„æƒ…å†µ**ï¼š

```
è¾“å…¥ x â†’ [Linear Layer] â†’ zâ‚ â†’ [ReLU] â†’ aâ‚ â†’ [Linear Layer] â†’ zâ‚‚ â†’ [Softmax] â†’ Å·
                                                                              â†“
                                                                         Loss L
```

è®¡ç®— $\frac{\partial L}{\partial W_1}$ï¼ˆç¬¬ä¸€å±‚æƒé‡å¯¹æŸå¤±çš„æ¢¯åº¦ï¼‰ï¼š

$$\frac{\partial L}{\partial W_1} = \frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z_2} \cdot \frac{\partial z_2}{\partial a_1} \cdot \frac{\partial a_1}{\partial z_1} \cdot \frac{\partial z_1}{\partial W_1}$$

è¿™æ­£æ˜¯**ä»å³åˆ°å·¦é€æ­¥ç›¸ä¹˜**çš„è¿‡ç¨‹ã€‚

### 2.2 ä¸€ä¸ªç®€å•ä¾‹å­

#### æ­£å‘ä¼ æ’­

ç»™å®šè¾“å…¥ $x = 2$ï¼Œæƒé‡ $W = 3$ï¼Œåç½® $b = 1$ï¼Œç›®æ ‡å€¼ $y = 10$ï¼š

```
æ­¥éª¤ 1ï¼šçº¿æ€§å±‚
  z = WÂ·x + b = 3Â·2 + 1 = 7

æ­¥éª¤ 2ï¼šæ¿€æ´»å‡½æ•°ï¼ˆReLUï¼‰
  a = max(0, z) = 7

æ­¥éª¤ 3ï¼šæŸå¤±å‡½æ•°ï¼ˆå¹³æ–¹è¯¯å·®ï¼‰
  L = 1/2Â·(a - y)Â² = 1/2Â·(7 - 10)Â² = 1/2Â·9 = 4.5
```

#### åå‘ä¼ æ’­

```
æ­¥éª¤ 1ï¼šæŸå¤±å¯¹æ¿€æ´»å€¼çš„æ¢¯åº¦
  âˆ‚L/âˆ‚a = a - y = 7 - 10 = -3

æ­¥éª¤ 2ï¼šæ¿€æ´»å¯¹ z çš„æ¢¯åº¦ï¼ˆReLU å¯¼æ•°ï¼‰
  âˆ‚a/âˆ‚z = 1ï¼ˆå› ä¸º z > 0ï¼ŒReLU çš„å¯¼æ•°æ˜¯ 1ï¼‰
         = 0ï¼ˆå¦‚æœ z â‰¤ 0ï¼‰

æ­¥éª¤ 3ï¼šz å¯¹ W çš„æ¢¯åº¦
  âˆ‚z/âˆ‚W = x = 2

æ­¥éª¤ 4ï¼šé“¾å¼ç›¸ä¹˜å¾—åˆ° âˆ‚L/âˆ‚W
  âˆ‚L/âˆ‚W = âˆ‚L/âˆ‚a Â· âˆ‚a/âˆ‚z Â· âˆ‚z/âˆ‚W = (-3) Â· 1 Â· 2 = -6
```

### 2.3 çŸ©é˜µå½¢å¼ï¼ˆé€‚ç”¨äºæ‰¹é‡å¤„ç†ï¼‰

#### å…¨è¿æ¥å±‚

**æ­£å‘**ï¼š
$$Z = XW + b$$
$$A = \text{ReLU}(Z)$$

å…¶ä¸­ $X$ æ˜¯ $(batch, d_{in})$ï¼Œ$W$ æ˜¯ $(d_{in}, d_{out})$ï¼Œ$Z$ æ˜¯ $(batch, d_{out})$ã€‚

**åå‘**ï¼š

ç»™å®š $\frac{\partial L}{\partial A}$ ï¼ˆä¸Šä¸€å±‚ä¼ æ¥çš„æ¢¯åº¦ï¼‰ï¼Œè®¡ç®— $\frac{\partial L}{\partial W}$ å’Œ $\frac{\partial L}{\partial b}$ï¼š

$$\frac{\partial L}{\partial Z} = \frac{\partial L}{\partial A} \odot \frac{\partial \text{ReLU}}{\partial Z}$$

å…¶ä¸­ $\odot$ è¡¨ç¤ºå…ƒç´ é€ä¸€ç›¸ä¹˜ï¼ˆHadamard ç§¯ï¼‰ã€‚

$$\frac{\partial L}{\partial W} = X^T \cdot \frac{\partial L}{\partial Z}$$

$$\frac{\partial L}{\partial b} = \text{sum}(\frac{\partial L}{\partial Z}, \text{axis}=0)$$

$$\frac{\partial L}{\partial X} = \frac{\partial L}{\partial Z} \cdot W^T$$

### 2.4 ä¸ºä»€ä¹ˆåå‘ä¼ æ’­è¿™ä¹ˆå¿«ï¼Ÿ

**æœ´ç´ æ–¹æ³•**ï¼šç”¨æ•°å€¼æ¢¯åº¦é€ä¸ªè®¡ç®—æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦ã€‚

```
å¯¹äº n ä¸ªå‚æ•°ï¼š
  for i in range(n):
    âˆ‚L/âˆ‚Î¸_i â‰ˆ (L(Î¸_i + Îµ) - L(Î¸_i - Îµ)) / 2Îµ

æ—¶é—´å¤æ‚åº¦ï¼šO(n Ã— forward_time)
```

å¯¹äºä¸€ä¸ªæœ‰ 10 äº¿å‚æ•°çš„æ¨¡å‹ï¼Œè¿™éœ€è¦ 10 äº¿æ¬¡æ­£å‘ä¼ æ’­ï¼

**åå‘ä¼ æ’­æ–¹æ³•**ï¼š

```
1 æ¬¡åå‘ä¼ æ’­ï¼Œè®¡ç®—æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦ã€‚

æ—¶é—´å¤æ‚åº¦ï¼šO(forward_time + backward_time) â‰ˆ O(3 Ã— forward_time)
```

è¿™æ˜¯ä¸€ä¸ª **4 é˜¶é‡çº§çš„åŠ é€Ÿ**ï¼ˆä» 10^9 å€åˆ°å¸¸æ•°å€ï¼‰ï¼

---

## 3. ğŸ”¬ åŸç†å®ç° (NumPy from Scratch)

### å®Œæ•´çš„åå‘ä¼ æ’­å®ç°

```python
import numpy as np
from typing import Tuple, List

class Layer:
    """åŸºç¡€å±‚ç±»ï¼Œå®šä¹‰å‰å‘å’Œåå‘ä¼ æ’­æ¥å£"""

    def forward(self, X: np.ndarray) -> np.ndarray:
        raise NotImplementedError

    def backward(self, dL_dA: np.ndarray) -> np.ndarray:
        """åå‘ä¼ æ’­ï¼Œè¿”å›ä¸Šä¸€å±‚çš„æ¢¯åº¦"""
        raise NotImplementedError

    def get_gradients(self) -> dict:
        """è¿”å›æœ¬å±‚å‚æ•°çš„æ¢¯åº¦"""
        return {}

    def update(self, learning_rate: float):
        """æ›´æ–°å‚æ•°"""
        pass

class LinearLayer(Layer):
    """å…¨è¿æ¥å±‚"""

    def __init__(self, in_features: int, out_features: int):
        self.W = np.random.randn(in_features, out_features) * np.sqrt(2.0 / in_features)
        self.b = np.zeros((1, out_features))

        # ç¼“å­˜ç”¨äºåå‘ä¼ æ’­
        self.X = None
        self.dL_dW = None
        self.dL_db = None

    def forward(self, X: np.ndarray) -> np.ndarray:
        """
        å‰å‘ä¼ æ’­ï¼šZ = XW + b
        X: (batch_size, in_features)
        W: (in_features, out_features)
        Z: (batch_size, out_features)
        """
        self.X = X
        return np.dot(X, self.W) + self.b

    def backward(self, dL_dZ: np.ndarray) -> np.ndarray:
        """
        åå‘ä¼ æ’­
        dL_dZ: (batch_size, out_features)

        è®¡ç®—ï¼š
        1. dL/dW = X^T Â· dL/dZ
        2. dL/db = sum(dL/dZ, axis=0)
        3. dL/dX = dL/dZ Â· W^Tï¼ˆä¼ ç»™ä¸Šä¸€å±‚ï¼‰
        """
        batch_size = self.X.shape[0]

        # è®¡ç®—æƒé‡å’Œåç½®çš„æ¢¯åº¦
        self.dL_dW = np.dot(self.X.T, dL_dZ)  # (in_features, out_features)
        self.dL_db = np.sum(dL_dZ, axis=0, keepdims=True)  # (1, out_features)

        # è®¡ç®—ä¼ ç»™ä¸Šä¸€å±‚çš„æ¢¯åº¦
        dL_dX = np.dot(dL_dZ, self.W.T)  # (batch_size, in_features)

        return dL_dX

    def get_gradients(self) -> dict:
        return {"dL_dW": self.dL_dW, "dL_db": self.dL_db}

    def update(self, learning_rate: float):
        """æ¢¯åº¦ä¸‹é™æ›´æ–°å‚æ•°"""
        self.W -= learning_rate * self.dL_dW
        self.b -= learning_rate * self.dL_db

class ReLU(Layer):
    """ReLU æ¿€æ´»å‡½æ•°"""

    def __init__(self):
        self.Z = None

    def forward(self, Z: np.ndarray) -> np.ndarray:
        """A = max(0, Z)"""
        self.Z = Z
        return np.maximum(0, Z)

    def backward(self, dL_dA: np.ndarray) -> np.ndarray:
        """
        dL/dZ = dL/dA Â· dA/dZ
        dA/dZ = 1 if Z > 0 else 0
        """
        dL_dZ = dL_dA * (self.Z > 0).astype(float)
        return dL_dZ

class SoftmaxWithCrossEntropy(Layer):
    """Softmax + äº¤å‰ç†µæŸå¤±ï¼ˆè”åˆè®¡ç®—ä»¥æ•°å€¼ç¨³å®šï¼‰"""

    def __init__(self):
        self.y_true = None
        self.y_pred = None

    def forward(self, Z: np.ndarray, y_true: np.ndarray) -> float:
        """
        Z: (batch_size, num_classes) - logits
        y_true: (batch_size, num_classes) - one-hot ç¼–ç 

        è¿”å›ï¼šå¹³å‡äº¤å‰ç†µæŸå¤±
        """
        batch_size = Z.shape[0]

        # æ•°å€¼ç¨³å®šçš„ softmax
        Z_shifted = Z - np.max(Z, axis=1, keepdims=True)
        exp_Z = np.exp(Z_shifted)
        self.y_pred = exp_Z / np.sum(exp_Z, axis=1, keepdims=True)

        self.y_true = y_true

        # äº¤å‰ç†µæŸå¤±
        loss = -np.sum(y_true * np.log(self.y_pred + 1e-8)) / batch_size

        return loss

    def backward(self) -> np.ndarray:
        """
        dL/dZ = y_pred - y_true

        è¿™æ˜¯ä¸€ä¸ªå·§å¦™çš„æ€§è´¨ï¼šsoftmax + cross-entropy çš„å¯¼æ•°éå¸¸ç®€æ´ï¼
        """
        return (self.y_pred - self.y_true)

class NeuralNetwork:
    """ç®€å•çš„å‰é¦ˆç¥ç»ç½‘ç»œ"""

    def __init__(self, layer_sizes: List[int]):
        """
        layer_sizes: [input_dim, hidden1, hidden2, ..., output_dim]
        """
        self.layers = []

        # æ„é€ ç½‘ç»œ
        for i in range(len(layer_sizes) - 1):
            self.layers.append(LinearLayer(layer_sizes[i], layer_sizes[i + 1]))
            if i < len(layer_sizes) - 2:  # æœ€åä¸€å±‚å‰ä¸åŠ æ¿€æ´»
                self.layers.append(ReLU())

        self.loss_layer = SoftmaxWithCrossEntropy()

    def forward(self, X: np.ndarray) -> np.ndarray:
        """å‰å‘ä¼ æ’­"""
        A = X
        for layer in self.layers:
            A = layer.forward(A)
        return A

    def backward(self, y_true: np.ndarray):
        """åå‘ä¼ æ’­"""
        # æŸå¤±å±‚åå‘
        dL_dZ = self.loss_layer.backward()  # (batch_size, num_classes)

        # ä»åå¾€å‰é€å±‚åå‘ä¼ æ’­
        for layer in reversed(self.layers):
            dL_dZ = layer.backward(dL_dZ)

    def train(
        self,
        X_train: np.ndarray,
        y_train: np.ndarray,
        learning_rate: float = 0.01,
        epochs: int = 100,
        batch_size: int = 32
    ) -> List[float]:
        """è®­ç»ƒç½‘ç»œ"""
        losses = []
        num_batches = len(X_train) // batch_size

        for epoch in range(epochs):
            epoch_loss = 0

            for i in range(num_batches):
                # è·å–å°æ‰¹æ¬¡
                start_idx = i * batch_size
                end_idx = start_idx + batch_size
                X_batch = X_train[start_idx:end_idx]
                y_batch = y_train[start_idx:end_idx]

                # å‰å‘ä¼ æ’­
                logits = self.forward(X_batch)

                # è®¡ç®—æŸå¤±
                loss = self.loss_layer.forward(logits, y_batch)
                epoch_loss += loss

                # åå‘ä¼ æ’­
                self.backward(y_batch)

                # å‚æ•°æ›´æ–°
                for layer in self.layers:
                    if isinstance(layer, LinearLayer):
                        layer.update(learning_rate)

            avg_loss = epoch_loss / num_batches
            losses.append(avg_loss)

            if (epoch + 1) % 10 == 0:
                print(f"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}")

        return losses

    def predict(self, X: np.ndarray) -> np.ndarray:
        """é¢„æµ‹"""
        logits = self.forward(X)
        return np.argmax(logits, axis=1)

# æ¼”ç¤ºï¼šè®­ç»ƒä¸€ä¸ªç®€å•çš„åˆ†ç±»ç½‘ç»œ
if __name__ == "__main__":
    # ç”Ÿæˆç®€å•çš„æ•°æ®é›†ï¼ˆXOR é—®é¢˜ï¼‰
    X_train = np.array([
        [0, 0],
        [0, 1],
        [1, 0],
        [1, 1]
    ], dtype=np.float32)

    y_train = np.array([
        [1, 0],  # 0 XOR 0 = 0
        [0, 1],  # 0 XOR 1 = 1
        [0, 1],  # 1 XOR 0 = 1
        [1, 0]   # 1 XOR 1 = 0
    ], dtype=np.float32)

    # åˆ›å»ºç½‘ç»œï¼š2 -> 4 -> 2
    model = NeuralNetwork([2, 4, 2])

    print("è®­ç»ƒå¼€å§‹...")
    losses = model.train(
        X_train,
        y_train,
        learning_rate=0.1,
        epochs=100,
        batch_size=4
    )

    print("\næµ‹è¯•é¢„æµ‹...")
    predictions = model.predict(X_train)
    print(f"é¢„æµ‹ç»“æœ: {predictions}")
    print(f"çœŸå®æ ‡ç­¾: {np.argmax(y_train, axis=1)}")
```

### ä»£ç è¯¦è§£

**å…³é”®ç‚¹ 1ï¼šç¼“å­˜è¾“å…¥**

```python
def forward(self, X):
    self.X = X  # â† ä¿å­˜è¾“å…¥ç”¨äºåå‘ä¼ æ’­
    return np.dot(X, self.W) + self.b
```

ä¸ºä»€ä¹ˆï¼Ÿåå‘ä¼ æ’­éœ€è¦ç”¨åˆ° $X$ æ¥è®¡ç®— $\frac{\partial L}{\partial W} = X^T \cdot \frac{\partial L}{\partial Z}$ã€‚

**å…³é”®ç‚¹ 2ï¼šé“¾å¼ç›¸ä¹˜**

```python
def backward(self, dL_dZ):
    # dL/dW = X^T Â· dL/dZ
    self.dL_dW = np.dot(self.X.T, dL_dZ)
    # dL/dX = dL/dZ Â· W^Tï¼ˆä¼ ç»™ä¸Šä¸€å±‚ï¼‰
    return np.dot(dL_dZ, self.W.T)
```

è¿™æ­£æ˜¯é“¾å¼æ³•åˆ™çš„å®ç°ã€‚

**å…³é”®ç‚¹ 3ï¼šSoftmax + Cross-Entropy çš„å·§å¦™æ€§**

```python
def backward(self):
    # dL/dZ = y_pred - y_true
    return (self.y_pred - self.y_true)
```

è¿™æ˜¯ä¸€ä¸ªæ•°å­¦ä¸Šçš„å¹¸è¿å·§åˆï¼š

$$L = -\sum_i y_i \log(\text{softmax}_i(Z))$$

$$\frac{\partial L}{\partial Z_j} = \text{softmax}_j(Z) - y_j$$

å¯¼æ•°éå¸¸ç®€æ´ï¼è¿™å°±æ˜¯ä¸ºä»€ä¹ˆåœ¨å®é™…ä¸­æŠŠ softmax å’Œ cross-entropy è”åˆå®ç°ã€‚

---

## 4. ğŸ§  å…³é”®æœºåˆ¶å‰–æ (Deep Dive)

### Q1: ä¸ºä»€ä¹ˆåå‘ä¼ æ’­ä¼šæ¯”æ­£å‘ä¼ æ’­å¿«ï¼Ÿ

æ•°å­¦ä¸Šï¼Œåå‘ä¼ æ’­å’Œæ­£å‘ä¼ æ’­çš„æˆæœ¬æ˜¯å¯¹ç§°çš„ï¼š

```
æ­£å‘ï¼šZ = XWï¼Œéœ€è¦ d_in Ã— d_out æ¬¡ä¹˜æ³•
åå‘ï¼šdL/dW = X^T Â· dL/dZï¼Œéœ€è¦ d_in Ã— d_out æ¬¡ä¹˜æ³•
```

**å®é™…ä¸Šå®ƒä»¬**å‡ ä¹ä¸€æ ·å¿«ï¼ˆç•¥æ…¢ä¸€ç‚¹ï¼Œå› ä¸ºæœ‰é¢å¤–çš„å†…å­˜è®¿é—®ï¼‰ã€‚

çœŸæ­£çš„åŠ é€Ÿæ¥è‡ªäº**å¤ç”¨è®¡ç®—ç»“æœ**ï¼š

```
æ•°å€¼æ¢¯åº¦æ³•ï¼ˆæœ´ç´ ï¼‰ï¼š
  éœ€è¦ n æ¬¡æ­£å‘ä¼ æ’­æ¥è®¡ç®— n ä¸ªå‚æ•°çš„æ¢¯åº¦

åå‘ä¼ æ’­ï¼š
  1 æ¬¡æ­£å‘ + 1 æ¬¡åå‘ï¼Œè®¡ç®—æ‰€æœ‰ n ä¸ªæ¢¯åº¦
```

### Q2: æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸çš„æ ¹æœ¬åŸå› æ˜¯ä»€ä¹ˆï¼Ÿ

```
å‡è®¾ä¸€ä¸ª 100 å±‚çš„ç½‘ç»œï¼Œæ¯ä¸€å±‚éƒ½æ˜¯çº¿æ€§å˜æ¢ï¼š
  âˆ‚L/âˆ‚W_1 = âˆ‚L/âˆ‚W_100 Â· (âˆ‚W_100/âˆ‚...) Â· ... Â· (âˆ‚W_2/âˆ‚W_1)
```

å¦‚æœæ¯ä¸€é¡¹éƒ½å°äº 1ï¼Œå°±ä¼šæŒ‡æ•°çº§è¡°å‡ï¼ˆæ¢¯åº¦æ¶ˆå¤±ï¼‰ã€‚
å¦‚æœæ¯ä¸€é¡¹éƒ½å¤§äº 1ï¼Œå°±ä¼šæŒ‡æ•°çº§å¢é•¿ï¼ˆæ¢¯åº¦çˆ†ç‚¸ï¼‰ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼š
- æ®‹å·®è¿æ¥ï¼ˆæ¢¯åº¦å¯ä»¥ç›´é€šï¼‰
- æ‰¹å½’ä¸€åŒ–ï¼ˆæ§åˆ¶æ¯å±‚è¾“å‡ºåˆ†å¸ƒï¼‰
- ç²¾å¿ƒé€‰æ‹©æ¿€æ´»å‡½æ•°ï¼ˆReLU æ¯” sigmoid æ›´å¥½ï¼‰

### Q3: ä¸ºä»€ä¹ˆéœ€è¦æ‰¹å¤„ç†ï¼ˆBatchï¼‰ï¼Ÿ

```
å•æ ·æœ¬ï¼šdL/dW æ˜¯ä¸€ä¸ªçŸ©é˜µ
æ‰¹å¤„ç†ï¼šdL/dW = X^T Â· dL/dZï¼Œå…¶ä¸­ X æ˜¯ (batch, d_in)

çŸ©é˜µä¹˜æ³•çš„å¥½å¤„ï¼š
- åˆ©ç”¨ GPU çš„çŸ©é˜µè®¡ç®—å•å…ƒï¼ˆGEMMï¼‰
- å¹¶è¡ŒåŒ–ç¨‹åº¦é«˜ï¼Œæ•ˆç‡è¿œè¶…é€ä¸ªæ ·æœ¬å¤„ç†

å®é™…åŠ é€Ÿï¼š50-100 å€ï¼ˆå–å†³äºç¡¬ä»¶ï¼‰
```

### Q4: ä¸ºä»€ä¹ˆè¦ç”¨å°å­¦ä¹ ç‡ï¼Ÿ

```
æ¢¯åº¦ä¸‹é™æ›´æ–°ï¼šÎ¸ â† Î¸ - Î±âˆ‡L

å¦‚æœ Î± å¤ªå¤§ï¼ˆæ¯”å¦‚ Î±=1ï¼‰ï¼š
  æ–°å‚æ•°å¯èƒ½è·³è¿‡æœ€ä¼˜å€¼ï¼Œç”šè‡³ä½¿æŸå¤±å¢å¤§

å¦‚æœ Î± å¤ªå°ï¼ˆæ¯”å¦‚ Î±=0.0001ï¼‰ï¼š
  æ”¶æ•›å¤ªæ…¢ï¼Œè®­ç»ƒæ—¶é—´é•¿

å®è·µç»éªŒï¼šÎ± âˆˆ [0.001, 0.1]
```

### é¢è¯•è€ƒç‚¹

1. **"ä¸ºä»€ä¹ˆ sigmoid ä¼šå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±ï¼Ÿ"**
   - $\sigma(x) = 1/(1+e^{-x})$
   - $\sigma'(x) = \sigma(x)(1-\sigma(x)) \leq 0.25$
   - 100 å±‚ï¼š$(0.25)^{100} \approx 0$ï¼ˆå®Œå…¨æ¶ˆå¤±ï¼ï¼‰

2. **"ReLU æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿ"**
   - **Dead ReLU**ï¼šå½“ Z < 0 æ—¶ï¼Œå¯¼æ•°ä¸º 0ï¼Œç¥ç»å…ƒæ°¸è¿œä¸ä¼šè¢«æ¿€æ´»
   - è§£å†³ï¼šä½¿ç”¨ Leaky ReLUï¼Œ$\text{LeakyReLU}(x) = \max(0.01x, x)$

3. **"æ¢¯åº¦å‰ªè£ï¼ˆGradient Clippingï¼‰çš„ä½œç”¨ï¼Ÿ"**
   - åœ¨åå‘ä¼ æ’­åï¼Œå°†æ¢¯åº¦çš„èŒƒæ•°é™åˆ¶åœ¨æŸä¸ªå€¼ä»¥ä¸‹
   - é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼ˆå°¤å…¶åœ¨ RNN ä¸­å¸¸è§ï¼‰

4. **"ä¸ºä»€ä¹ˆæˆ‘çš„æŸå¤± NaN äº†ï¼Ÿ"**
   - åŸå›  1ï¼šå­¦ä¹ ç‡å¤ªå¤§ï¼Œå‚æ•°æ›´æ–°è¿‡åº¦
   - åŸå›  2ï¼šåˆå§‹åŒ–ä¸å½“ï¼Œå¯¼è‡´æ¿€æ´»è¿‡å¤§æˆ–è¿‡å°
   - åŸå›  3ï¼šè¾“å…¥æ•°æ®æ²¡æœ‰å½’ä¸€åŒ–

---

## 5. ğŸ”— çŸ¥è¯†è¿æ¥

### åŸºç¡€æ¦‚å¿µ

- **[[Gradient_Descent]]** - æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ç®—æ³•
- **[[Activation_Functions]]** - ReLU, Sigmoid, Tanh ç­‰é€‰æ‹©
- **[[Batch_Normalization]]** - åŠ é€Ÿè®­ç»ƒçš„æŠ€å·§

### é«˜çº§ä¼˜åŒ–

- **[[Adam_Optimizer]]** - è‡ªé€‚åº”å­¦ä¹ ç‡ä¼˜åŒ–å™¨
- **[[Mixed_Precision_Training]]** - åŠ é€Ÿåå‘ä¼ æ’­
- **[[Automatic_Differentiation]]** - PyTorch/TensorFlow å¦‚ä½•è‡ªåŠ¨è®¡ç®—æ¢¯åº¦

### å¸¸è§é—®é¢˜

- **[[Vanishing_Gradient_Problem]]** - æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
- **[[Weight_Initialization]]** - æƒé‡åˆå§‹åŒ–å¯¹è®­ç»ƒçš„å½±å“

---

## ğŸ“Š åå‘ä¼ æ’­çš„æ—¶é—´å¤æ‚åº¦

| æ“ä½œ | æ—¶é—´å¤æ‚åº¦ | è¯´æ˜ |
|------|-----------|------|
| æ­£å‘ä¼ æ’­ | O(å‚æ•°æ•°é‡) | æ¯ä¸ªå‚æ•°åšä¸€æ¬¡ä¹˜æ³• |
| åå‘ä¼ æ’­ | O(å‚æ•°æ•°é‡) | é“¾å¼æ³•åˆ™é€å±‚è®¡ç®— |
| æ•°å€¼æ¢¯åº¦ï¼ˆæœ´ç´ ï¼‰ | O(å‚æ•°æ•°é‡ Ã— å‚æ•°æ•°é‡) | éœ€è¦ n æ¬¡æ­£å‘ä¼ æ’­ |

**å¯ç¤º**ï¼šåå‘ä¼ æ’­æ˜¯æ·±åº¦å­¦ä¹ å¾—ä»¥å¯è¡Œçš„å…³é”®ç®—æ³•ã€‚æ²¡æœ‰å®ƒï¼Œè®­ç»ƒå¤§è§„æ¨¡ç¥ç»ç½‘ç»œåœ¨è®¡ç®—ä¸Šæ˜¯ä¸å¯è¡Œçš„ã€‚

---

## ğŸ“š æ¨èèµ„æº

1. **ç»å…¸è®ºæ–‡**ï¼šã€ŠLearning Representations by Back-propagating Errorsã€‹(Rumelhart et al., 1986)
2. **å…¥é—¨è®²è§£**ï¼š3Blue1Brown çš„ç¥ç»ç½‘ç»œç³»åˆ—è§†é¢‘
3. **è¯¦ç»†æ¨å¯¼**ï¼šAndrej Karpathy çš„ CS231n è®²ä¹‰

---

**æœ€åçš„ç›´è§‰**ï¼š

åå‘ä¼ æ’­æ˜¯ä¸€ç§**åŠ¨æ€è§„åˆ’**æ€æƒ³åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨ã€‚å®ƒé¿å…äº†é‡å¤è®¡ç®—ï¼Œé€šè¿‡ä¸€æ¬¡æ­£å‘ä¼ æ’­å’Œä¸€æ¬¡åå‘ä¼ æ’­ï¼Œåœ¨ O(n) æ—¶é—´å†…è®¡ç®— n ä¸ªå‚æ•°çš„æ¢¯åº¦ã€‚è¿™ä¸ªä¼˜é›…çš„ç®—æ³•ä½¿å¾—è®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œæˆä¸ºäº†å¯èƒ½ã€‚
