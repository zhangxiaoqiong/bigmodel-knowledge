---
topic: Transformer (Transformer æ¶æ„)
category: Architecture
difficulty: â­â­â­â­â­
tags: [åŸºç¡€, æ¶æ„, å¿…ä¿®, æ ¸å¿ƒ]
---

# ğŸ—ï¸ Transformer Architecture

## 1. ğŸ“– æœ¬è´¨å®šä¹‰ (First Principles)

> [!SUMMARY] æ¦‚å¿µå¡ç‰‡
>
> - **å®šä¹‰**ï¼šTransformer æ˜¯ä¸€ç§çº¯ç²¹åŸºäº**è‡ªæ³¨æ„åŠ›æœºåˆ¶**ï¼ˆSelf-Attentionï¼‰çš„æ·±åº¦å­¦ä¹ æ¶æ„ï¼Œç”¨äºå¤„ç†åºåˆ—æ•°æ®ã€‚å®ƒç”±ç¼–ç å™¨ï¼ˆEncoderï¼‰å’Œè§£ç å™¨ï¼ˆDecoderï¼‰ä¸¤ä¸ªå †å çš„ç›¸åŒæ¨¡å—ç»„æˆï¼Œæ¯ä¸ªæ¨¡å—åŒ…å«å¤šå¤´æ³¨æ„åŠ›å±‚å’Œå‰é¦ˆç½‘ç»œå±‚ã€‚
>
> - **å†å²èƒŒæ™¯**ï¼šåœ¨ Transformer ä¹‹å‰ï¼ŒRNN/LSTM æ˜¯åºåˆ—å¤„ç†çš„æ ‡å‡†æ–¹æ³•ï¼Œä½†å®ƒä»¬æœ‰ä¸¤ä¸ªè‡´å‘½å¼±ç‚¹ï¼š(1) **é¡ºåºå¤„ç†**å¯¼è‡´æ— æ³•å¹¶è¡ŒåŒ–ï¼Œè®­ç»ƒé€Ÿåº¦æ…¢ï¼›(2) **é•¿è·ç¦»æ¢¯åº¦**è¡°å‡ï¼Œéš¾ä»¥å­¦ä¹ é•¿æ–‡æœ¬ä¸­çš„ä¾èµ–å…³ç³»ã€‚Transformer é€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶æ‰“ç ´äº†é¡ºåºçº¦æŸï¼Œä½¿å¾—æ‰€æœ‰ä½ç½®å¯ä»¥å¹¶è¡Œè®¡ç®—ã€‚
>
> - **æ ¸å¿ƒç›´è§‰**ï¼šä¸å…¶è®©ä¸€ä¸ª"éšçŠ¶æ€"åƒä¼ é€å¸¦ä¸€æ ·é€æ­¥ä¼ é€’ä¿¡æ¯ï¼Œä¸å¦‚è®©æ¯ä¸ªä½ç½®éƒ½èƒ½ç›´æ¥"æŸ¥è¯¢"å…¶ä»–æ‰€æœ‰ä½ç½®ã€‚è¿™å°±æ˜¯è‡ªæ³¨æ„åŠ›çš„æ ¸å¿ƒã€‚

### ğŸ¯ è®¾è®¡ç›®æ ‡

| ç›®æ ‡ | è§£å†³çš„é—®é¢˜ | Transformer æ–¹æ¡ˆ |
|------|-----------|-----------------|
| **å¹¶è¡ŒåŒ–** | RNN å¿…é¡»é¡ºåºè®¡ç®—ï¼Œæ— æ³• GPU åŠ é€Ÿ | æ‰€æœ‰ä½ç½®åŒæ—¶è®¡ç®—æ³¨æ„åŠ›ï¼ŒO(log n) æ·±åº¦ |
| **é•¿ä¾èµ–** | LSTM æ— æ³•æœ‰æ•ˆæ•æ‰é•¿è·ç¦»å…³ç³» | æ¯ä¸ªä½ç½®éƒ½èƒ½ç›´æ¥ä¸æ‰€æœ‰ä½ç½®äº¤äº’ |
| **å¯æ‰©å±•æ€§** | éš¾ä»¥å¤„ç†è¶…é•¿æ–‡æœ¬ | çº¿æ€§å †å å±‚æ•°ï¼Œæ¨¡å‹å®¹é‡å¯æ§ |

---

## 2. ğŸ“ æ•´ä½“æ¶æ„ (The Math)

### 2.1 ç¼–ç å™¨-è§£ç å™¨æ¡†æ¶

```
è¾“å…¥åºåˆ—ï¼š[token_1, token_2, ..., token_n]
            |
            v
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   ç¼–ç å™¨ (Encoder)      â”‚  â† ç†è§£è¾“å…¥
        â”‚  â”Œâ”€ Multi-Head Att      â”‚
        â”‚  â”œâ”€ Feed Forward        â”‚
        â”‚  â””â”€ (é‡å¤ N æ¬¡)         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            |
            v
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  è§£ç å™¨ (Decoder)       â”‚  â† ç”Ÿæˆè¾“å‡º
        â”‚  â”Œâ”€ Masked Multi-Head   â”‚
        â”‚  â”œâ”€ Cross-Attention     â”‚
        â”‚  â”œâ”€ Feed Forward        â”‚
        â”‚  â””â”€ (é‡å¤ N æ¬¡)         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            |
            v
        è¾“å‡ºåºåˆ—ï¼š[token_1, token_2, ..., token_m]
```

### 2.2 å•å±‚ï¼ˆTransformer Blockï¼‰çš„ç»†èŠ‚

#### ç¼–ç å™¨å±‚ (Encoder Block)

```
è¾“å…¥ x_l: (batch, seq_len, d_model)
    |
    v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ å¤šå¤´è‡ªæ³¨æ„åŠ›å±‚                 â”‚
â”‚ x_l' = MultiHeadAttention(    â”‚
â”‚        x_l, x_l, x_l) + x_l   â”‚â† æ®‹å·®è¿æ¥
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    |
    v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ å±‚å½’ä¸€åŒ– (LayerNorm)          â”‚
â”‚ y = LayerNorm(x_l')           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    |
    v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ å‰é¦ˆç½‘ç»œ (Feed-Forward)       â”‚
â”‚ FFN(y) = ReLU(yW_1 + b_1)W_2  â”‚
â”‚          + b_2 + y            â”‚â† æ®‹å·®è¿æ¥
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    |
    v
è¾“å‡º x_{l+1}: (batch, seq_len, d_model)
```

#### æ•°å­¦è¡¨è¾¾

$$\text{Encoder}(x_l) = \text{FFN}(\text{LayerNorm}(\text{MultiHeadAtt}(x_l) + x_l)) + \text{MultiHeadAtt}(x_l)$$

**æ³¨æ„**ï¼šè®ºæ–‡ä¸­çš„é¡ºåºæ˜¯ LayerNorm åœ¨å‰ï¼ˆPre-Normï¼‰ï¼Œä¹Ÿæœ‰æ”¹è¿›ç‰ˆæ˜¯åœ¨åï¼ˆPost-Normï¼‰ã€‚

#### å…³é”®æ¦‚å¿µè¯´æ˜

| æ¦‚å¿µ | ä½œç”¨ | ä¸ºä»€ä¹ˆéœ€è¦å®ƒ |
|------|------|-----------|
| **æ®‹å·®è¿æ¥** (Residual) | $\text{output} = f(x) + x$ | é˜²æ­¢æ·±å±‚ç½‘ç»œæ¢¯åº¦æ¶ˆå¤±ï¼Œå…è®¸æ¢¯åº¦ç›´é€š |
| **å±‚å½’ä¸€åŒ–** (LayerNorm) | å°† $x$ å½’ä¸€åŒ–ä¸ºå‡å€¼ 0ã€æ–¹å·® 1 | ç¨³å®šè®­ç»ƒï¼ŒåŠ é€Ÿæ”¶æ•› |
| **å‰é¦ˆç½‘ç»œ** | ä¸¤å±‚å…¨è¿æ¥ï¼š$d_{model} \to d_{ff} \to d_{model}$ | å¼•å…¥éçº¿æ€§ï¼Œé€šå¸¸ $d_{ff} \approx 4 \times d_{model}$ |

### 2.3 å‰é¦ˆç½‘ç»œçš„ç»†èŠ‚

$$\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2$$

**ä¸ºä»€ä¹ˆç”¨ä¸¤å±‚ï¼Ÿ**

- ç¬¬ä¸€å±‚ï¼š$d_{model} \to d_{ff}$ï¼ˆé€šå¸¸ $d_{ff} = 4 \times d_{model}$ï¼Œå³ 2048ï¼‰
  - ç›®çš„ï¼š**æŠ•å½±åˆ°é«˜ç»´ç©ºé—´**ï¼Œå¢åŠ è¡¨è¾¾èƒ½åŠ›
  - å¼•å…¥ ReLU éçº¿æ€§

- ç¬¬äºŒå±‚ï¼š$d_{ff} \to d_{model}$ï¼ˆé™ç»´å›åŸå§‹ç»´åº¦ï¼‰
  - ç›®çš„ï¼š**æŠ•å½±å›åŸå§‹ç©ºé—´**ï¼Œç¡®ä¿ä¸åç»­å±‚å…¼å®¹

**å‚æ•°é‡è®¡ç®—**ï¼š
$$\text{Parameters}_{FFN} = d_{model} \times d_{ff} + d_{ff} \times d_{model}$$
$$= 2 \times d_{model} \times d_{ff} = 2 \times d_{model} \times 4 \times d_{model} = 8 d_{model}^2$$

å¯¹äº $d_{model} = 768$ï¼ˆBERT å¤§å°ï¼‰ï¼š
$$= 8 \times 768^2 \approx 4.7 \text{ ç™¾ä¸‡å‚æ•°}$$

å‰é¦ˆç½‘ç»œé€šå¸¸å  Transformer æ€»å‚æ•°çš„ **60-70%**ã€‚

### 2.4 è§£ç å™¨å±‚çš„ç‰¹æ®Šä¹‹å¤„

è§£ç å™¨åœ¨ç¼–ç å™¨åŸºç¡€ä¸Šå¢åŠ äº†ä¸€ä¸ª**äº¤å‰æ³¨æ„åŠ›**å±‚ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. è‡ªæ³¨æ„åŠ› (Masked)                 â”‚  â† åªèƒ½çœ‹å·²ç”Ÿæˆçš„ token
â”‚    Att(x, x, x, mask=causal)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    |
    v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. äº¤å‰æ³¨æ„åŠ› (Cross-Attention)     â”‚  â† æŸ¥è¯¢ç¼–ç å™¨è¾“å‡º
â”‚    Att(decoder, encoder, encoder)   â”‚
â”‚    Q æ¥è‡ªè§£ç å™¨ï¼ŒKã€V æ¥è‡ªç¼–ç å™¨    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    |
    v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. å‰é¦ˆç½‘ç»œ                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. ğŸ”¬ åŸç†å®ç° (NumPy from Scratch)

### å®Œæ•´ Transformer å®ç°

```python
import numpy as np
from typing import Optional, Tuple

def softmax(x: np.ndarray, axis: int = -1) -> np.ndarray:
    """æ•°å€¼ç¨³å®šçš„ softmax"""
    x_shifted = x - np.max(x, axis=axis, keepdims=True)
    exp_x = np.exp(x_shifted)
    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)

def layer_norm(x: np.ndarray, eps: float = 1e-6) -> np.ndarray:
    """å±‚å½’ä¸€åŒ–"""
    mean = np.mean(x, axis=-1, keepdims=True)
    std = np.std(x, axis=-1, keepdims=True)
    return (x - mean) / (std + eps)

class PositionalEncoding:
    """ä½ç½®ç¼–ç ï¼šä¸ºæ¯ä¸ªä½ç½®æ·»åŠ ç‹¬ç‰¹çš„ä¿¡å·"""

    def __init__(self, d_model: int, max_seq_len: int = 5000):
        """
        PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
        PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
        """
        pe = np.zeros((max_seq_len, d_model))
        position = np.arange(0, max_seq_len).reshape(-1, 1)  # (max_seq_len, 1)
        div_term = np.exp(
            np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model)
        )  # (d_model/2,)

        pe[:, 0::2] = np.sin(position * div_term)  # å¶æ•°ä½ï¼šsin
        pe[:, 1::2] = np.cos(position * div_term)  # å¥‡æ•°ä½ï¼šcos

        self.pe = pe

    def __call__(self, seq_len: int) -> np.ndarray:
        """è¿”å›å‰ seq_len çš„ä½ç½®ç¼–ç """
        return self.pe[:seq_len, :]

class MultiHeadAttention:
    """å¤šå¤´æ³¨æ„åŠ›"""

    def __init__(self, d_model: int, num_heads: int):
        assert d_model % num_heads == 0
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads

        # åˆå§‹åŒ–æŠ•å½±çŸ©é˜µï¼ˆå®é™…åº”è¯¥é€šè¿‡è®­ç»ƒå­¦ä¹ ï¼‰
        self.W_q = np.random.randn(d_model, d_model) * 0.01
        self.W_k = np.random.randn(d_model, d_model) * 0.01
        self.W_v = np.random.randn(d_model, d_model) * 0.01
        self.W_o = np.random.randn(d_model, d_model) * 0.01

    def scaled_dot_product_attention(
        self,
        Q: np.ndarray,
        K: np.ndarray,
        V: np.ndarray,
        mask: Optional[np.ndarray] = None
    ) -> Tuple[np.ndarray, np.ndarray]:
        """å•å¤´æ³¨æ„åŠ›"""
        scores = np.matmul(Q, K.transpose(0, 2, 1))  # (batch, seq_len, seq_len)
        scores = scores / np.sqrt(self.d_k)

        if mask is not None:
            scores = np.where(mask, scores, -1e9)

        attention_weights = softmax(scores, axis=-1)
        output = np.matmul(attention_weights, V)  # (batch, seq_len, d_v)

        return output, attention_weights

    def __call__(
        self,
        Q: np.ndarray,
        K: np.ndarray,
        V: np.ndarray,
        mask: Optional[np.ndarray] = None
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        å‚æ•°ï¼š
            Q, K, V: (batch_size, seq_len, d_model)
            mask: (batch_size, 1, seq_len, seq_len) æˆ– (batch_size, num_heads, seq_len, seq_len)
        """
        batch_size = Q.shape[0]

        # çº¿æ€§æŠ•å½±å¹¶åˆ†å‰²ä¸ºå¤šä¸ªå¤´
        Q = np.matmul(Q, self.W_q).reshape(
            batch_size, -1, self.num_heads, self.d_k
        ).transpose(0, 2, 1, 3)  # (batch, num_heads, seq_len, d_k)
        K = np.matmul(K, self.W_k).reshape(
            batch_size, -1, self.num_heads, self.d_k
        ).transpose(0, 2, 1, 3)
        V = np.matmul(V, self.W_v).reshape(
            batch_size, -1, self.num_heads, self.d_k
        ).transpose(0, 2, 1, 3)

        # å¯¹æ¯ä¸ªå¤´è®¡ç®—æ³¨æ„åŠ›
        attn_output, attention_weights = self.scaled_dot_product_attention(
            Q, K, V, mask
        )  # (batch, num_heads, seq_len, d_k)

        # è¿æ¥æ‰€æœ‰å¤´
        attn_output = attn_output.transpose(0, 2, 1, 3).reshape(
            batch_size, -1, self.d_model
        )  # (batch, seq_len, d_model)

        # æœ€ç»ˆçº¿æ€§æŠ•å½±
        output = np.matmul(attn_output, self.W_o)

        return output, attention_weights

class FeedForwardNetwork:
    """å‰é¦ˆç½‘ç»œ"""

    def __init__(self, d_model: int, d_ff: int):
        self.W_1 = np.random.randn(d_model, d_ff) * 0.01
        self.b_1 = np.zeros((1, d_ff))
        self.W_2 = np.random.randn(d_ff, d_model) * 0.01
        self.b_2 = np.zeros((1, d_model))

    def __call__(self, x: np.ndarray) -> np.ndarray:
        """
        FFN(x) = max(0, xW_1 + b_1)W_2 + b_2
        """
        hidden = np.maximum(0, np.matmul(x, self.W_1) + self.b_1)  # ReLU
        output = np.matmul(hidden, self.W_2) + self.b_2
        return output

class TransformerEncoderBlock:
    """Transformer ç¼–ç å™¨å±‚"""

    def __init__(self, d_model: int, num_heads: int, d_ff: int):
        self.attention = MultiHeadAttention(d_model, num_heads)
        self.ffn = FeedForwardNetwork(d_model, d_ff)

    def __call__(
        self,
        x: np.ndarray,
        self_attn_mask: Optional[np.ndarray] = None
    ) -> np.ndarray:
        """
        x: (batch_size, seq_len, d_model)
        """
        # å¤šå¤´æ³¨æ„åŠ› + æ®‹å·®
        attn_output, _ = self.attention(x, x, x, self_attn_mask)
        x = x + attn_output

        # å±‚å½’ä¸€åŒ–
        x = layer_norm(x)

        # å‰é¦ˆç½‘ç»œ + æ®‹å·®
        ffn_output = self.ffn(x)
        x = x + ffn_output

        # å±‚å½’ä¸€åŒ–
        x = layer_norm(x)

        return x

class TransformerEncoder:
    """Transformer ç¼–ç å™¨ï¼ˆå¤šå±‚å †å ï¼‰"""

    def __init__(
        self,
        d_model: int,
        num_layers: int,
        num_heads: int,
        d_ff: int
    ):
        self.layers = [
            TransformerEncoderBlock(d_model, num_heads, d_ff)
            for _ in range(num_layers)
        ]
        self.pos_encoding = PositionalEncoding(d_model)

    def __call__(self, x: np.ndarray) -> np.ndarray:
        """
        x: (batch_size, seq_len, d_model)
        """
        seq_len = x.shape[1]

        # åŠ ä¸Šä½ç½®ç¼–ç 
        pos_enc = self.pos_encoding(seq_len)  # (seq_len, d_model)
        x = x + pos_enc[np.newaxis, :, :]  # å¹¿æ’­åˆ° batch

        # é€šè¿‡æ‰€æœ‰å±‚
        for layer in self.layers:
            x = layer(x)

        return x

# æµ‹è¯•ç¤ºä¾‹
if __name__ == "__main__":
    batch_size, seq_len, d_model = 2, 10, 64
    num_heads, num_layers, d_ff = 8, 2, 256

    # åˆ›å»ºéšæœºè¾“å…¥
    X = np.random.randn(batch_size, seq_len, d_model)

    # åˆ›å»º Encoder
    encoder = TransformerEncoder(d_model, num_layers, num_heads, d_ff)

    # å‰å‘ä¼ æ’­
    output = encoder(X)

    print(f"è¾“å…¥å½¢çŠ¶: {X.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"è¾“å‡ºèŒƒå›´: [{output.min():.4f}, {output.max():.4f}]")
```

### å…³é”®å®ç°ç»†èŠ‚

1. **ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰**ï¼š
   - ä½¿ç”¨æ­£å¼¦å’Œä½™å¼¦å‡½æ•°
   - ä¸åŒé¢‘ç‡çš„æ³¢å½¢ç¼–ç ä¸åŒçš„ä½ç½®
   - ä¼˜ç‚¹ï¼šå¯å¤–æ¨åˆ°æ›´é•¿çš„åºåˆ—

2. **æ®‹å·®è¿æ¥ï¼ˆResidual Connectionï¼‰**ï¼š
   - $x_{l+1} = \text{SubLayer}(x_l) + x_l$
   - å¥½å¤„ï¼šæ¢¯åº¦å¯ä»¥ç›´æ¥åå‘ä¼ æ’­

3. **å±‚å½’ä¸€åŒ–ï¼ˆLayer Normalizationï¼‰**ï¼š
   - å¯¹æœ€åä¸€ç»´ï¼ˆç‰¹å¾ç»´ï¼‰è¿›è¡Œå½’ä¸€åŒ–
   - ç¨³å®šè®­ç»ƒï¼ŒåŠ é€Ÿæ”¶æ•›

---

## 4. ğŸ§  å…³é”®æœºåˆ¶å‰–æ (Deep Dive)

### Q1: ä¸ºä»€ä¹ˆéœ€è¦ä½ç½®ç¼–ç ï¼Ÿ

**é—®é¢˜**ï¼šSelf-Attention æ˜¯**æ’åˆ—ä¸å˜**çš„ã€‚

```python
x = [token_1, token_2, token_3]
x_shuffled = [token_3, token_1, token_2]

# è™½ç„¶é¡ºåºæ”¹å˜ï¼Œä½†è‡ªæ³¨æ„åŠ›çš„è®¡ç®—ç»“æœæ˜¯ä¸€æ ·çš„ï¼
# å› ä¸ºæ³¨æ„åŠ›åªå…³å¿ƒ"å“ªäº› token ä¸å“ªäº› token ç›¸ä¼¼"ï¼Œ
# ä¸å…³å¿ƒå®ƒä»¬åœ¨åºåˆ—ä¸­çš„ä½ç½®
```

**è§£å†³æ–¹æ¡ˆ**ï¼šç›´æ¥æŠŠä½ç½®ä¿¡æ¯ç¼–ç åˆ°è¾“å…¥ä¸­ã€‚

$$\text{Positional Encoding}(pos, 2i) = \sin(pos / 10000^{2i/d_{model}})$$
$$\text{Positional Encoding}(pos, 2i+1) = \cos(pos / 10000^{2i/d_{model}})$$

**ä¸ºä»€ä¹ˆç”¨ä¸‰è§’å‡½æ•°ï¼Ÿ**

- å¯¹ä»»æ„å›ºå®šè·ç¦» $\delta$ï¼Œ$PE_{pos+\delta}$ å¯ä»¥è¡¨ç¤ºä¸º $PE_{pos}$ çš„çº¿æ€§å‡½æ•°
- æ¨¡å‹å¯ä»¥å­¦ä¼šç›¸å¯¹ä½ç½®å…³ç³»
- å¯ä»¥å¤–æ¨åˆ°æ›´é•¿çš„åºåˆ—ï¼ˆè¿™æ˜¯ç›¸å¯¹ä½ç½®ç¼–ç çš„ä¼˜åŠ¿ï¼‰

### Q2: ä¸ºä»€ä¹ˆå‰é¦ˆç½‘ç»œæœ‰ 4 å€çš„éšå±‚ï¼Ÿ

$$d_{ff} = 4 \times d_{model}$$

è¿™æ²¡æœ‰ä¸¥æ ¼çš„æ•°å­¦æ¨å¯¼ï¼Œè€Œæ˜¯ç»éªŒä¸Šå‘ç°çš„ï¼š

- å¤ªå°ï¼šè¡¨è¾¾èƒ½åŠ›ä¸è¶³
- å¤ªå¤§ï¼ˆå¦‚ 8 å€ï¼‰ï¼šå‚æ•°è¿‡å¤šï¼Œè®­ç»ƒå›°éš¾ï¼Œæ•ˆæœåè€Œä¸‹é™
- 4 å€ï¼šæœ€ä½³å¹³è¡¡ç‚¹

### Q3: ä¸ºä»€ä¹ˆç”¨å¤šå¤´æ³¨æ„åŠ›è€Œä¸æ˜¯å•å¤´çš„å¤§æ³¨æ„åŠ›çŸ©é˜µï¼Ÿ

```
å•å¤´ï¼š(d_model, d_model) çŸ©é˜µï¼Œå‚æ•° = d_modelÂ²
     ä½†çŸ©é˜µç§©æœ‰é™ï¼Œéš¾ä»¥æ•æ‰å¤šç§å…³ç³»

å¤šå¤´ï¼šh ä¸ª (d_k, d_k) çŸ©é˜µï¼Œæ€»å‚æ•° = h Ã— d_kÂ² = d_modelÂ²
     ä½†ä¸åŒçš„å¤´å­¦åˆ°ä¸åŒçš„æŠ•å½±ç©ºé—´

ç±»æ¯”ï¼šç”¨å¤šä¸ªä¸åŒç„¦è·çš„é•œå¤´è§‚å¯Ÿä¸–ç•Œï¼Œvs ç”¨ä¸€ä¸ªé•œå¤´
```

å®éªŒè¯æ˜ï¼Œå¤šå¤´æ³¨æ„åŠ›æ¯”å•å¤´æ€§èƒ½å¥½å¾—å¤šã€‚

### Q4: æ®‹å·®è¿æ¥çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ

```
æ²¡æœ‰æ®‹å·®è¿æ¥ï¼š
  x_0 ---> Layer_1 ---> Layer_2 ---> ... ---> Layer_N

æ¢¯åº¦åå‘ä¼ æ’­ï¼š
  âˆ‚L/âˆ‚x_0 = âˆ‚L/âˆ‚x_N Ã— âˆ‚x_N/âˆ‚x_{N-1} Ã— ... Ã— âˆ‚x_1/âˆ‚x_0

å¦‚æœæ¯ä¸ª âˆ‚x_i/âˆ‚x_{i-1} < 1ï¼Œæ¢¯åº¦å‘ˆæŒ‡æ•°è¡°å‡ï¼ˆæ¢¯åº¦æ¶ˆå¤±ï¼‰ï¼

æœ‰æ®‹å·®è¿æ¥ï¼š
  x_1 = f(x_0) + x_0

æ¢¯åº¦ï¼š
  âˆ‚x_1/âˆ‚x_0 = âˆ‚f/âˆ‚x_0 + 1
             â‰¥ 1ï¼ˆå³ä½¿ âˆ‚f/âˆ‚x_0 = 0ï¼Œæ¢¯åº¦ä¹Ÿèƒ½é€šè¿‡ "+1" é¡¹æµåŠ¨ï¼‰
```

è¿™æ˜¯ä¸ºä»€ä¹ˆç°åœ¨å¯ä»¥è®­ç»ƒå‡ ç™¾å±‚çš„æ·±åº¦ç½‘ç»œã€‚

### é¢è¯•è€ƒç‚¹

1. **"Transformer çš„æ—¶é—´å¤æ‚åº¦æ˜¯å¤šå°‘ï¼Ÿ"**
   - å¤šå¤´æ³¨æ„åŠ›ï¼šO(LÂ² Ã— d_{model})
   - å‰é¦ˆç½‘ç»œï¼šO(L Ã— d_{model}Â²)
   - æ€»ä½“ï¼šO(LÂ² Ã— d_{model}) ï¼ˆæ³¨æ„åŠ›æ˜¯ç“¶é¢ˆï¼‰

2. **"ä¸ºä»€ä¹ˆ Transformer æ¯” RNN æ›´æ˜“å¹¶è¡ŒåŒ–ï¼Ÿ"**
   - RNNï¼šç¬¬ t æ­¥ä¾èµ–ç¬¬ t-1 æ­¥ï¼Œå¿…é¡»é¡ºåºè®¡ç®—
   - Transformerï¼šæ‰€æœ‰ä½ç½®çš„æ³¨æ„åŠ›å¯ä»¥åŒæ—¶è®¡ç®—

3. **"Attention å’Œ Recurrence å“ªä¸ªæ›´é€‚åˆæ•æ‰é•¿è·ç¦»ä¾èµ–ï¼Ÿ"**
   - Attentionï¼šç›´æ¥è¿æ¥ï¼Œè·ç¦»ä¸º 1 æ­¥
   - Recurrenceï¼šè·ç¦»ä¸º t æ­¥ï¼Œéœ€è¦æ¢¯åº¦ç»è¿‡å¤šä¸ªæ—¶é—´æ­¥

4. **"è§£ç å™¨ä¸ºä»€ä¹ˆéœ€è¦ masked self-attentionï¼Ÿ"**
   - ç”Ÿæˆæ—¶ï¼Œç¬¬ t ä¸ª token ä¸èƒ½çœ‹åˆ°ç¬¬ t+1 ä¸ª token
   - å¦åˆ™å°±"ä½œå¼Š"äº†ï¼ˆå·çœ‹ç­”æ¡ˆï¼‰

---

## 5. ğŸ”— çŸ¥è¯†è¿æ¥

### æ ¸å¿ƒç»„ä»¶

- **[[Attention_Mechanism]]** - Transformer çš„çµé­‚
- **[[Position_Encoding]]** - ä½ç½®ä¿¡æ¯çš„ç¼–ç æ–¹å¼
- **[[Layer_Normalization]]** - è®­ç»ƒç¨³å®šæ€§ä¿éšœ

### è¡ç”Ÿæ¶æ„

- **[[BERT]]** - ä»…ç¼–ç å™¨çš„ Transformerï¼ˆåŒå‘ï¼‰
- **[[GPT]]** - ä»…è§£ç å™¨çš„ Transformerï¼ˆè‡ªå›å½’ï¼‰
- **[[Vision_Transformer_ViT]]** - å°†å›¾åƒåˆ†å—åç”¨ Transformer å¤„ç†

### ç°ä»£ä¼˜åŒ–

- **[[Flash_Attention]]** - é™ä½æ³¨æ„åŠ›è®¡ç®—å¤æ‚åº¦
- **[[Sparse_Attention]]** - é€‰æ‹©æ€§æ³¨æ„åŠ›
- **[[Grouped_Query_Attention_GQA]]** - å‡å°‘æ¨ç†å†…å­˜
- **[[MLA]]** - DeepSeek ä½¿ç”¨çš„æ”¹è¿›æ³¨æ„åŠ›

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

| æ¨¡å‹ | æ¶æ„ | è®­ç»ƒé€Ÿåº¦ | é•¿æ–‡æœ¬èƒ½åŠ› | æ¨ç†æˆæœ¬ |
|------|------|---------|---------|--------|
| LSTM | Recurrent | æ…¢ â­ | å¼± â­ | ä¸­ç­‰ |
| Transformer (åŸç‰ˆ) | Self-Attention | å¿« â­â­â­â­â­ | å¼º â­â­â­â­ | é«˜ â­â­â­â­â­ |
| Flash Attention | Optimized Attention | å¿« â­â­â­â­â­ | å¼º â­â­â­â­ | ä½ â­â­â­â­ |

---

## ğŸ“š æ¨èèµ„æº

1. **åŸå§‹è®ºæ–‡**ï¼šã€ŠAttention Is All You Needã€‹(Vaswani et al., 2017)
2. **è¯¦ç»†è®²è§£**ï¼šThe Illustrated Transformer (Jay Alammar)
3. **å®ç°å‚è€ƒ**ï¼šAnnotated Transformer (Alexander Rush)

---

**æœ€åçš„ç›´è§‰**ï¼š

Transformer æ˜¯æ·±åº¦å­¦ä¹ çš„ä¸€æ¬¡èŒƒå¼è½¬ç§»ã€‚å®ƒç”¨**æ³¨æ„åŠ›æ›¿ä»£å¾ªç¯**ï¼Œç”¨**å¹¶è¡Œæ›¿ä»£é¡ºåº**ã€‚è¿™ä½¿å¾—æ¨¡å‹å¯ä»¥åŒæ—¶çœ‹åˆ°æ•´ä¸ªåºåˆ—ï¼Œå¹¶ä¸”å¯ä»¥æ·±åº¦å åŠ è€Œä¸æŸå¤±æ¢¯åº¦ã€‚è¿™ä¸ªç®€å•è€Œä¼˜é›…çš„è®¾è®¡æˆä¸ºäº†æ‰€æœ‰ç°ä»£å¤§æ¨¡å‹çš„åŸºçŸ³ã€‚
