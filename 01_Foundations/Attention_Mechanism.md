---
topic: Attention Mechanism (æ³¨æ„åŠ›æœºåˆ¶)
category: Architecture
difficulty: â­â­â­â­
tags: [åŸºç¡€, åŸç†, å¿…ä¿®, Transformer]
---

# ğŸ‘ï¸ Attention Mechanism (æ³¨æ„åŠ›æœºåˆ¶)

## 1. ğŸ“– æœ¬è´¨å®šä¹‰ (First Principles)

> [!SUMMARY] æ¦‚å¿µå¡ç‰‡
>
> - **å®šä¹‰**ï¼šæ³¨æ„åŠ›æœºåˆ¶æ˜¯ä¸€ç§è®¡ç®—æ–¹æ³•ï¼Œé€šè¿‡å­¦ä¹ è¾“å…¥åºåˆ—ä¸­ä¸åŒä½ç½®çš„**å…³è”å¼ºåº¦**ï¼ŒåŠ¨æ€åœ°åŠ æƒç»„åˆè¾“å…¥ä¿¡æ¯ï¼Œç”Ÿæˆä¸Šä¸‹æ–‡ç›¸å…³çš„è¡¨ç¤ºã€‚æ•°å­¦ä¸Šæ˜¯ï¼šç»™å®šæŸ¥è¯¢å‘é‡ Qã€é”®å‘é‡ K å’Œå€¼å‘é‡ Vï¼Œè®¡ç®— Q ä¸ K çš„ç›¸ä¼¼åº¦åˆ†å¸ƒï¼Œç”¨è¯¥åˆ†å¸ƒå¯¹ V è¿›è¡ŒåŠ æƒæ±‚å’Œã€‚
>
> - **å†å²èƒŒæ™¯**ï¼šåœ¨ RNN/LSTM æ—¶ä»£ï¼Œæ¨¡å‹åªèƒ½ä¾èµ–**çŸ­æœŸè®°å¿†**å’Œ**éšçŠ¶æ€**ä¼ é€’æ¥æ•æ‰é•¿è·ç¦»ä¾èµ–ã€‚ä½†éšçŠ¶æ€çš„å®¹é‡æœ‰é™ï¼Œå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±å’Œä¿¡æ¯ç“¶é¢ˆã€‚Attention çªç ´æ€§åœ°å…è®¸æ¯ä¸ªä½ç½®ç›´æ¥ä¸å…¶ä»–æ‰€æœ‰ä½ç½®é€šä¿¡ï¼Œæ‰“ç ´äº†åºåˆ—å¤„ç†çš„é¡ºåºé™åˆ¶ã€‚
>
> - **æ ¸å¿ƒç›´è§‰**ï¼šå°±åƒäººç±»é˜…è¯»æ—¶ï¼Œçœ¼ç›ä¸ä¼šå¹³ç­‰åœ°æ³¨è§†æ¯ä¸ªè¯ï¼Œè€Œæ˜¯æ ¹æ®ä»»åŠ¡åŠ¨æ€åœ°èšç„¦äºå…³é”®è¯ã€‚Attention è®©æ¨¡å‹å­¦ä¼š"çœ‹å“ªé‡Œ"ã€‚

### ğŸ“Š é—®é¢˜è®¾å®š

åœ¨æ²¡æœ‰ Attention ä¹‹å‰ï¼ŒRNN å¤„ç†é•¿åºåˆ—é¢ä¸´çš„å›°å¢ƒï¼š

```
è¾“å…¥åºåˆ—ï¼š[token_1, token_2, ..., token_100]
  |
  v
RNN: h_1 -> h_2 -> ... -> h_100
     ^^^^   ^^^^         ^^^^

é—®é¢˜ï¼šè¦é¢„æµ‹ token_50ï¼Œæ¨¡å‹å¿…é¡»é€šè¿‡ h_1â†’h_2â†’...â†’h_50 çš„ä¿¡æ¯ä¼ é€’é“¾ã€‚
     è¿™æ¡é•¿é“¾ä¸­æ¯ä¸€æ­¥éƒ½æœ‰æ¢¯åº¦è¡°å‡é£é™© (æ¢¯åº¦æ¶ˆå¤±é—®é¢˜)
```

**Attention çš„è§£å†³æ–¹æ¡ˆ**ï¼šè®© token_50 èƒ½å¤Ÿç›´æ¥"æŸ¥è¯¢" token_1 åˆ° token_49ï¼Œé€‰æ‹©æœ€ç›¸å…³çš„ä¸Šä¸‹æ–‡ã€‚

---

## 2. ğŸ“ æ•°å­¦æ¨å¯¼ (The Math)

### 2.1 Scaled Dot-Product Attention (ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›)

è¿™æ˜¯ç°ä»£ Transformer ä½¿ç”¨çš„æ ‡å‡†å½¢å¼ã€‚

#### æ ¸å¿ƒå…¬å¼

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

#### é€é¡¹è§£é‡Š

| ç¬¦å· | å«ä¹‰ | ç»´åº¦ | ä¸ºä»€ä¹ˆéœ€è¦å®ƒ |
|------|------|------|-----------|
| $Q$ | Query (æŸ¥è¯¢çŸ©é˜µ) | $(L, d_k)$ | è¡¨ç¤º"æˆ‘è¦æ‰¾ä»€ä¹ˆä¿¡æ¯" |
| $K$ | Key (é”®çŸ©é˜µ) | $(L, d_k)$ | è¡¨ç¤º"æˆ‘æœ‰ä»€ä¹ˆä¿¡æ¯" |
| $V$ | Value (å€¼çŸ©é˜µ) | $(L, d_v)$ | è¡¨ç¤º"å®é™…çš„ä¿¡æ¯å†…å®¹" |
| $d_k$ | é”®ç»´åº¦ | æ ‡é‡ | æ¨¡å‹å†…éƒ¨éšå±‚å¤§å° |
| $L$ | åºåˆ—é•¿åº¦ | æ ‡é‡ | è¾“å…¥æœ‰å¤šå°‘ä¸ª token |
| $\sqrt{d_k}$ | **ç¼©æ”¾å› å­** | æ ‡é‡ | é˜²æ­¢ç‚¹ç§¯è¿‡å¤§å¯¼è‡´æ¢¯åº¦æ¶ˆå¤± |

#### ä¸ºä»€ä¹ˆè¦æœ‰ $\sqrt{d_k}$ ç¼©æ”¾ï¼Ÿ

å½“ $d_k$ å¾ˆå¤§æ—¶ï¼ˆæ¯”å¦‚ 512ï¼‰ï¼Œ$QK^T$ çš„å€¼ä¼šå¾ˆå¤§ã€‚

å‡è®¾ $Q$ å’Œ $K$ çš„å…ƒç´ æ˜¯æ ‡å‡†æ­£æ€åˆ†å¸ƒï¼š
- ç‚¹ç§¯çš„æœŸæœ›ï¼š$E[Q \cdot K] = 0$
- ç‚¹ç§¯çš„æ–¹å·®ï¼š$\text{Var}[Q \cdot K] = d_k$

å¦‚æœä¸ç¼©æ”¾ï¼Œå½“ $d_k = 512$ æ—¶ï¼Œ$QK^T$ ä¸­çš„å€¼ä¼šè½åœ¨ $[-d_k, d_k]$ èŒƒå›´ï¼ˆå¤§çº¦ $[-22, 22]$ï¼‰ã€‚

Softmax çš„å¯¼æ•°æ˜¯ $\frac{\partial}{\partial x}\text{softmax}(x) = \text{softmax}(x)(1 - \text{softmax}(x))$ã€‚

å½“è¾“å…¥å¾ˆå¤§æ—¶ï¼Œsoftmax ä¼šé¥±å’Œåˆ°æ¥è¿‘ 0 æˆ– 1ï¼Œå¯¼è‡´æ¢¯åº¦æ¥è¿‘ 0ï¼ˆ**æ¢¯åº¦æ¶ˆå¤±**ï¼‰ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼šé™¤ä»¥ $\sqrt{d_k}$ åï¼Œç‚¹ç§¯è¢«æ ‡å‡†åŒ–åˆ° $[-1, 1]$ èŒƒå›´ï¼Œæ¢¯åº¦èƒ½å¤Ÿæ­£å¸¸æµåŠ¨ã€‚

#### åˆ†æ­¥è®¡ç®—ç¤ºæ„

```
ç¬¬ 1 æ­¥ï¼šè®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
  QK^T = (L, d_k) Ã— (d_k, L) = (L, L)

  è¿™ä¸ª (L, L) çŸ©é˜µçš„æ¯ä¸ªå…ƒç´  [i, j] è¡¨ç¤ºï¼š
  query_i ä¸ key_j çš„ç›¸ä¼¼åº¦ï¼ˆç‚¹ç§¯ï¼‰

ç¬¬ 2 æ­¥ï¼šç¼©æ”¾
  QK^T / âˆšd_k

  å°†ç›¸ä¼¼åº¦åˆ†å¸ƒæ ‡å‡†åŒ–ï¼Œä½¿å¾—æ¢¯åº¦æµæ›´ç¨³å®š

ç¬¬ 3 æ­¥ï¼šå½’ä¸€åŒ–ä¸ºæ¦‚ç‡
  softmax(QK^T / âˆšd_k) = (L, L)

  æ¯ä¸€è¡Œè¢«è½¬æ¢ä¸ºä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒ

  ä¾‹å¦‚ç¬¬ 1 è¡Œï¼š[0.05, 0.7, 0.1, 0.05, 0.1]
  è¡¨ç¤º query_1 åº”è¯¥å…³æ³¨ key çš„åˆ†å¸ƒ

ç¬¬ 4 æ­¥ï¼šåŠ æƒæ±‚å’Œ
  Attention_output = softmax(...) Ã— V = (L, L) Ã— (L, d_v) = (L, d_v)

  ç”¨æ³¨æ„åŠ›æƒé‡åŠ æƒ V çš„å€¼
```

### 2.2 å¤šå¤´æ³¨æ„åŠ› (Multi-Head Attention)

å•ä¸ªæ³¨æ„åŠ›å¤´å¯èƒ½ä¸è¶³ä»¥æ•æ‰æ‰€æœ‰ç±»å‹çš„å…³ç³»ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªå¤´å­¦ä¼šäº†"è¯­æ³•å…³ç³»"ï¼Œå¦ä¸€ä¸ªå¤´å­¦ä¼šäº†"è¯­ä¹‰å…³ç³»"ã€‚

$$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O$$

å…¶ä¸­æ¯ä¸ªå¤´ï¼š

$$\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$

#### ä¸ºä»€ä¹ˆè¦å¤šå¤´ï¼Ÿ

å‡è®¾ $d_{model} = 512$ï¼Œ$h = 8$ å¤´ï¼š
- æ¯ä¸ªå¤´çš„ç»´åº¦ï¼š$d_k = d_v = 512 / 8 = 64$
- æ¯ä¸ªå¤´å­¦ä¹ è¾“å…¥ç©ºé—´çš„ä¸€ä¸ª"æŠ•å½±"

**è¡¨è¾¾åŠ›å¢åŠ **ï¼šè€Œä¸æ˜¯ç”¨å•ä¸ª (512, 512) çŸ©é˜µåšæ³¨æ„åŠ›ï¼Œè€Œæ˜¯ç”¨ 8 ä¸ª (64, 64) çš„å°çŸ©é˜µã€‚è¿™ç›¸å½“äºç”¨ 8 ä¸ª"é€é•œ"ä»ä¸åŒè§’åº¦è§‚å¯Ÿæ•°æ®ã€‚

ä¾‹å­ï¼š
- Head 1ï¼šä¸“æ³¨äº"ä¸»è¯­-è°“è¯­"å…³ç³»ï¼ˆè¯­æ³•ï¼‰
- Head 2ï¼šä¸“æ³¨äº"ä»£è¯æŒ‡ä»£"ï¼ˆè¯­ä¹‰ï¼‰
- Head 3ï¼šä¸“æ³¨äº"ä¿®é¥°å…³ç³»"ï¼ˆè¯­ä¹‰ï¼‰
- ...

---

## 3. ğŸ”¬ åŸç†å®ç° (NumPy from Scratch)

### åŸºç¡€å®ç°

```python
import numpy as np

def softmax(x, axis=-1):
    """æ•°å€¼ç¨³å®šçš„ softmax å®ç°"""
    # å‡å»æœ€å¤§å€¼é˜²æ­¢æº¢å‡º
    x_shifted = x - np.max(x, axis=axis, keepdims=True)
    exp_x = np.exp(x_shifted)
    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)

def scaled_dot_product_attention(Q, K, V, mask=None):
    """
    å•å¤´æ³¨æ„åŠ›æœºåˆ¶çš„çº¯ NumPy å®ç°

    å‚æ•°ï¼š
        Q: (seq_len, d_k) - æŸ¥è¯¢çŸ©é˜µ
        K: (seq_len, d_k) - é”®çŸ©é˜µ
        V: (seq_len, d_v) - å€¼çŸ©é˜µ
        mask: (seq_len, seq_len) - å¯é€‰çš„æ©ç çŸ©é˜µï¼ˆç”¨äºå› æœæ³¨æ„åŠ›ï¼‰

    è¿”å›ï¼š
        output: (seq_len, d_v) - æ³¨æ„åŠ›è¾“å‡º
        attention_weights: (seq_len, seq_len) - æ³¨æ„åŠ›æƒé‡
    """
    d_k = Q.shape[-1]

    # æ­¥éª¤ 1ï¼šè®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
    scores = np.matmul(Q, K.T)  # (seq_len, seq_len)

    # æ­¥éª¤ 2ï¼šç¼©æ”¾
    scores = scores / np.sqrt(d_k)

    # æ­¥éª¤ 3ï¼šåº”ç”¨æ©ç ï¼ˆå¦‚æœæœ‰ï¼‰
    if mask is not None:
        scores = np.where(mask, scores, -1e9)

    # æ­¥éª¤ 4ï¼šSoftmax å½’ä¸€åŒ–
    attention_weights = softmax(scores, axis=-1)

    # æ­¥éª¤ 5ï¼šåŠ æƒæ±‚å’Œ
    output = np.matmul(attention_weights, V)  # (seq_len, d_v)

    return output, attention_weights

def multi_head_attention(Q, K, V, num_heads, d_model, mask=None):
    """
    å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶çš„å®ç°

    å‚æ•°ï¼š
        Q, K, V: (seq_len, d_model) - è¾“å…¥
        num_heads: å¤´çš„æ•°é‡
        d_model: æ¨¡å‹ç»´åº¦
        mask: å¯é€‰çš„æ©ç 

    è¿”å›ï¼š
        output: (seq_len, d_model) - å¤šå¤´æ³¨æ„åŠ›è¾“å‡º
    """
    assert d_model % num_heads == 0, "d_model å¿…é¡»èƒ½è¢« num_heads æ•´é™¤"

    d_k = d_model // num_heads
    seq_len = Q.shape[0]

    # åˆå§‹åŒ–æŠ•å½±çŸ©é˜µï¼ˆç®€åŒ–ï¼šç”¨éšæœºçŸ©é˜µä»£æ›¿è®­ç»ƒå‚æ•°ï¼‰
    W_Q = np.random.randn(d_model, d_model) * 0.01
    W_K = np.random.randn(d_model, d_model) * 0.01
    W_V = np.random.randn(d_model, d_model) * 0.01
    W_O = np.random.randn(d_model, d_model) * 0.01

    # æŠ•å½±åˆ°å¤šå¤´ç©ºé—´
    Q_proj = np.matmul(Q, W_Q)  # (seq_len, d_model)
    K_proj = np.matmul(K, W_K)
    V_proj = np.matmul(V, W_V)

    # åˆ†å‰²ä¸ºå¤šä¸ªå¤´
    heads = []
    for h in range(num_heads):
        Q_h = Q_proj[:, h*d_k:(h+1)*d_k]  # (seq_len, d_k)
        K_h = K_proj[:, h*d_k:(h+1)*d_k]
        V_h = V_proj[:, h*d_k:(h+1)*d_k]

        # è®¡ç®—å•å¤´æ³¨æ„åŠ›
        head_output, _ = scaled_dot_product_attention(Q_h, K_h, V_h, mask)
        heads.append(head_output)

    # è¿æ¥æ‰€æœ‰å¤´
    concat_heads = np.concatenate(heads, axis=-1)  # (seq_len, d_model)

    # æœ€ç»ˆçº¿æ€§æŠ•å½±
    output = np.matmul(concat_heads, W_O)

    return output

# æµ‹è¯•ç”¨ä¾‹
if __name__ == "__main__":
    seq_len, d_model = 10, 64

    # åˆ›å»ºéšæœºè¾“å…¥
    X = np.random.randn(seq_len, d_model)

    # å•å¤´æ³¨æ„åŠ›
    output_single, weights = scaled_dot_product_attention(X, X, X)
    print(f"å•å¤´æ³¨æ„åŠ›è¾“å‡ºå½¢çŠ¶: {output_single.shape}")
    print(f"æ³¨æ„åŠ›æƒé‡æ ·æœ¬ (ç¬¬ä¸€è¡Œ):\n{weights[0]}")
    print(f"æ³¨æ„åŠ›æƒé‡å’Œ: {weights[0].sum():.6f}")  # åº”è¯¥æ˜¯ 1.0

    # å¤šå¤´æ³¨æ„åŠ›
    output_multi = multi_head_attention(X, X, X, num_heads=8, d_model=d_model)
    print(f"\nå¤šå¤´æ³¨æ„åŠ›è¾“å‡ºå½¢çŠ¶: {output_multi.shape}")
```

### ä»£ç è¯¦è§£

**ä¸ºä»€ä¹ˆç”¨ `np.max(x) - x` åš softmaxï¼Ÿ**

å½“è®¡ç®— softmax æ—¶ï¼Œå¦‚æœ $x$ å¤ªå¤§ï¼Œ$e^x$ ä¼šæº¢å‡ºï¼ˆè¶…å‡ºæµ®ç‚¹æ•°èŒƒå›´ï¼‰ã€‚

é€šè¿‡å‡å»æœ€å¤§å€¼ï¼š
$$\text{softmax}(x_i) = \frac{e^{x_i - \max(x)}}{\sum_j e^{x_j - \max(x)}}$$

åˆ†å­åˆ†æ¯éƒ½é™¤ä»¥åŒä¸€ä¸ªå€¼ï¼ˆ$e^{-\max(x)}$ï¼‰ï¼Œç»“æœä¸å˜ï¼Œä½†æ•°å€¼ç¨³å®šã€‚

**ä¸ºä»€ä¹ˆè¦åˆ†å‰²ä¸ºå¤šä¸ªå¤´ï¼Ÿ**

- å•ä¸ª 64Ã—64 çš„æ³¨æ„åŠ›çŸ©é˜µå®¹æ˜“è¿‡æ‹Ÿåˆ
- å¤šä¸ªå°çŸ©é˜µå¯ä»¥å­¦ä¹ æ•°æ®çš„ä¸åŒ"æŠ•å½±"
- å‚æ•°æ€»æ•°ä¸å˜ï¼Œä½†è¡¨è¾¾åŠ›æ›´å¼ºï¼ˆç±»ä¼¼äº CNN ä¸­çš„å¤šé€šé“ï¼‰

---

## 4. ğŸ§  å…³é”®æœºåˆ¶å‰–æ (Deep Dive)

### What if... (å¦‚æœä¸è¿™æ ·åšä¼šæ€æ ·ï¼Ÿ)

#### Q1: å¦‚æœä¸ç¼©æ”¾ $\sqrt{d_k}$ï¼Ÿ

```
d_k = 512
ä¸ç¼©æ”¾çš„ç‚¹ç§¯ï¼šQK^T çš„å€¼èŒƒå›´çº¦ [-22, 22]

softmax(22) â‰ˆ 0.999999
softmax(-22) â‰ˆ 0.000001

âˆ‚softmax/âˆ‚x â‰ˆ 0.000001 Ã— (1 - 0.000001) â‰ˆ 0

é—®é¢˜ï¼šåå‘ä¼ æ’­æ—¶ï¼Œæ¢¯åº¦æµä¸º 0ï¼Œæ¨¡å‹æ— æ³•å­¦ä¹ ï¼
```

#### Q2: å¦‚æœä¸ç”¨å¤šå¤´ï¼Ÿ

```
ç”¨å•ä¸ªå¤§çŸ©é˜µ (d_model, d_model) åšæ³¨æ„åŠ›ï¼š
- å®¹æ˜“è¿‡å‚æ•°åŒ–å’Œè¿‡æ‹Ÿåˆ
- æ— æ³•å­¦ä¹ å¤šç§å…³ç³»ç±»å‹
- ç±»æ¯”ï¼šç”¨ä¸€ç§é¢œè‰²çœ‹ä¸–ç•Œï¼Œvs ç”¨ RGB ä¸‰è‰²

å¤šå¤´çš„æ•ˆæœï¼šå…è®¸æ¨¡å‹åŒæ—¶å­¦ä¹ å¤šä¸ªå­ç©ºé—´çš„æ³¨æ„åŠ›æ¨¡å¼
```

#### Q3: ä»€ä¹ˆæ˜¯å› æœæ³¨æ„åŠ›ï¼ˆCausal Attentionï¼‰ï¼Ÿ

åœ¨è‡ªå›å½’ç”Ÿæˆä¸­ï¼ˆæ¯”å¦‚ç”Ÿæˆä¸‹ä¸€ä¸ª tokenï¼‰ï¼Œæ¨¡å‹ä¸èƒ½"çœ‹åˆ°æœªæ¥"ã€‚

```python
# åˆ›å»ºå› æœæ©ç 
seq_len = 10
mask = np.triu(np.ones((seq_len, seq_len)), k=1).astype(bool)
# æ©ç çš„å½¢å¼ï¼š
# [F F F F F]  ä½ç½® 0 åªèƒ½çœ‹è‡ªå·±
# [F F F F F]  ä½ç½® 1 èƒ½çœ‹ä½ç½® 0, 1
# [F T F F F]  ä½ç½® 2 èƒ½çœ‹ä½ç½® 0, 1, 2
# [F T T F F]  ä½ç½® 3 èƒ½çœ‹ä½ç½® 0, 1, 2, 3
# ...

# ä½¿ç”¨æ©ç ï¼šmask ä¸­ä¸º True çš„ä½ç½®è¢«è®¾ä¸º -1e9ï¼Œsoftmax åå˜ä¸º 0
```

### é¢è¯•è€ƒç‚¹

1. **"ä¸ºä»€ä¹ˆ Transformer æ¯” RNN å¿«ï¼Ÿ"**
   - RNN å¿…é¡»é€æ­¥è®¡ç®— h_1, h_2, ..., h_Tï¼ˆé¡ºåºä¾èµ–ï¼‰
   - Transformer å¯ä»¥å¹¶è¡Œè®¡ç®—æ‰€æœ‰ä½ç½®çš„æ³¨æ„åŠ›ï¼ˆæ²¡æœ‰é¡ºåºä¾èµ–ï¼‰
   - ä½†å†…å­˜å ç”¨æ˜¯ O(LÂ²)ï¼ŒRNN æ˜¯ O(L)

2. **"Attention çš„æ—¶é—´å¤æ‚åº¦æ˜¯å¤šå°‘ï¼Ÿ"**
   - è®¡ç®— QK^T: O(LÂ² Ã— d_k)
   - è®¡ç®— softmax: O(LÂ²)
   - æ€»è®¡ï¼šO(LÂ² Ã— d_k)
   - è¿™æ˜¯ä¸ºä»€ä¹ˆé•¿æ–‡æœ¬å¤„ç†å›°éš¾çš„æ ¹æœ¬åŸå› 

3. **"Self-attention vs Cross-attention çš„åŒºåˆ«ï¼Ÿ"**
   - Self-attention: Q, K, V éƒ½æ¥è‡ªåŒä¸€è¾“å…¥
   - Cross-attention: Q æ¥è‡ªä¸€ä¸ªè¾“å…¥ï¼ŒK, V æ¥è‡ªå¦ä¸€ä¸ªè¾“å…¥
   - ç”¨é€”ï¼šåºåˆ—å†…éƒ¨å…³ç³» vs å¤šè¾“å…¥èåˆ

4. **"ä¸ºä»€ä¹ˆéœ€è¦ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰ï¼Ÿ"**
   - Attention æ˜¯**æ’åˆ—ä¸å˜**çš„ï¼ˆæ¢ä¸ªé¡ºåºï¼Œç»“æœä¸å˜ï¼‰
   - ä½†åºåˆ—çš„é¡ºåºå¾ˆé‡è¦ï¼
   - éœ€è¦é¢å¤–çš„ä½ç½®ä¿¡æ¯ç¼–ç åˆ°è¾“å…¥ä¸­

---

## 5. ğŸ”— çŸ¥è¯†è¿æ¥

### è¡ç”ŸæŠ€æœ¯

- **[[Transformer]]** - æ³¨æ„åŠ›æœºåˆ¶æ˜¯ Transformer çš„æ ¸å¿ƒç»„ä»¶
- **[[Multi-Head_Attention_Variants]]** - Flash Attention, Sparse Attention ç­‰ä¼˜åŒ–
- **[[Position_Encoding]]** - Attention æœºåˆ¶çš„è¡¥å……ï¼ˆæä¾›ä½ç½®ä¿¡æ¯ï¼‰
- **[[DeepSeek_V3]]** - ä½¿ç”¨ MLA (Multi-Head Latent Attention) æ”¹è¿›ç‰ˆæœ¬çš„æ³¨æ„åŠ›

### å†å²é“¾æ¡

```
2017: Attention is All You Need è®ºæ–‡ï¼ˆVaswani et al.ï¼‰
  â†“
2017: Transformer æ¶æ„æå‡º
  â†“
2018: BERT (åŒå‘æ³¨æ„åŠ›)
  â†“
2020: Efficient Transformers (æ”¹è¿›æ³¨æ„åŠ›å¤æ‚åº¦)
  â†“
2024: Flash Attention 2, MLA ç­‰ä¼˜åŒ–ç‰ˆæœ¬
```

---

## ğŸ“š æ¨èé˜…è¯»

1. **åŸå§‹è®ºæ–‡**ï¼šã€ŠAttention Is All You Needã€‹(Vaswani et al., 2017)
   - ç¬¬ 3 èŠ‚ï¼šAttention æœºåˆ¶çš„å®Œæ•´æ¨å¯¼

2. **é«˜çº§è¯é¢˜**ï¼š
   - Linear Attention (é™ä½æ—¶é—´å¤æ‚åº¦)
   - Sparse Attention (é•¿æ–‡æœ¬ä¼˜åŒ–)
   - MLA (Multi-Head Latent Attention) - DeepSeek V3 ä½¿ç”¨

---

**æœ€åçš„ç›´è§‰**ï¼š

Attention çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ª**åŠ æƒå¹³å‡**æ“ä½œï¼Œæƒé‡ç”±æ¨¡å‹**å­¦ä¹ **è€Œéæ‰‹å·¥è®¾å®šã€‚å®ƒè®©æ¯ä¸ªä½ç½®éƒ½èƒ½"çœ‹åˆ°"æ•´ä¸ªåºåˆ—ï¼Œè€Œæƒé‡åˆ†å¸ƒç”±ä»»åŠ¡è‡ªåŠ¨å­¦ä¹ å†³å®šã€‚è¿™æ‰“ç ´äº† RNN çš„é¡ºåºçº¦æŸï¼Œæ˜¯ç°ä»£æ·±åº¦å­¦ä¹ çš„åŸºçŸ³ã€‚
